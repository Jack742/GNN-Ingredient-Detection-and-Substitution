{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_auc_score,precision_score, recall_score, f1_score\n",
    "\n",
    "#www.python-graph-gallery.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USAGE:\\nIf Training GNN -> set the following to true\\n|MAKE_TEST_SET\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\nNOTE: Cannot Test GNN unless model has already been trained or data is present\\nIf Testing GNN -> set the following to true\\n|LOAD_TEST_SET\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\nIf Training MLP/XGBOOST -> set the following to true\\n|USE_BRANDS_AND_INGR\\n|LOAD_BALANCED_DATA\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\nIf Testing MLP/XGBOOST -> set the following to true\\n|USE_BRANDS_AND_INGR\\n|LOAD_TEST_SET\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\n||ORDER OF RUNNING||\\n| RUN TRAIN-GNN FIRST, THEN TEST-GNN, THEN TRAIN MLP/XG, THEN TEST-MLP/XG\\n| ENSURE FRAC IS ALWAYS\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"USAGE:\n",
    "If Training GNN -> set the following to true\n",
    "|MAKE_TEST_SET\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "NOTE: Cannot Test GNN unless model has already been trained or data is present\n",
    "If Testing GNN -> set the following to true\n",
    "|LOAD_TEST_SET\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "If Training MLP/XGBOOST -> set the following to true\n",
    "|USE_BRANDS_AND_INGR\n",
    "|LOAD_BALANCED_DATA\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "If Testing MLP/XGBOOST -> set the following to true\n",
    "|USE_BRANDS_AND_INGR\n",
    "|LOAD_TEST_SET\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "||ORDER OF RUNNING||\n",
    "| RUN TRAIN-GNN FIRST, THEN TEST-GNN, THEN TRAIN MLP/XG, THEN TEST-MLP/XG\n",
    "| ENSURE FRAC IS ALWAYS\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNBALANCED_LINK_palm oil_20_percent_of_data_final_saved_balanced_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "INGREDIENT_TO_REPLACE = 'palm oil'#'palm oil'#'peanut'#palm oil'#'palm oil'#'soya'#'peanut'#'palm oil''peanut'#'wheat'#'wheat'#'soya'#\n",
    "\n",
    "\n",
    "MASK = False\n",
    "USE_FREQUENCY = False\n",
    "\n",
    "\n",
    "gnn_path = f'{INGREDIENT_TO_REPLACE}_link_pred_gnn'\n",
    "test_set_path = f'{INGREDIENT_TO_REPLACE}_link_test_set.csv'\n",
    "balanced_data_path = f'{INGREDIENT_TO_REPLACE})_link_final_saved_balanced_dataset.csv'\n",
    "\n",
    "LOAD_BALANCED_DATA = False\n",
    "\n",
    "\n",
    "LOAD_TEST_SET = True\n",
    "MAKE_TEST_SET = not LOAD_TEST_SET\n",
    "\n",
    "TEST_SAMPLE_FRAC = 0.08\n",
    "SAMPLE_FRAC = 0.2\n",
    "\n",
    "STEM = True\n",
    "DECOMPOSE_HIERARCHY = True\n",
    "REMOVE_REPLACE = False\n",
    "perc_for_print = str(SAMPLE_FRAC*100).split('.')[0]\n",
    "root_to_standard_data = f'UNBALANCED_LINK_{INGREDIENT_TO_REPLACE}_{perc_for_print}_percent_of_data_final_saved_balanced_dataset.csv'\n",
    "print(root_to_standard_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data()->pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Load data from file, depending on global config parameters set\n",
    "    \"\"\"   \n",
    "    if LOAD_BALANCED_DATA:\n",
    "        df = pd.read_csv(balanced_data_path)\n",
    "    elif LOAD_TEST_SET:\n",
    "        df = pd.read_csv(test_set_path)\n",
    "    else:\n",
    "        files = [\n",
    "            \"outputs/food_1_Y.csv\",\n",
    "            \"outputs/food_2_Y.csv\",\n",
    "            \"outputs/food_3_Y.csv\"\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for file in files:\n",
    "            df = pd.concat([df, pd.read_csv(file)])\n",
    "        \n",
    "        if MAKE_TEST_SET:\n",
    "            #df1 = df.sample(frac=TEST_SAMPLE_FRAC, random_state=123456789)\n",
    "            #df = df.drop(df1.index)\n",
    "            df = df.sample(frac=SAMPLE_FRAC, random_state=123456789)\n",
    "            #df1.reset_index()\n",
    "            df = df.reset_index()\n",
    "            df.to_csv(f'{INGREDIENT_TO_REPLACE}_link_test_set.csv')\n",
    "        else:\n",
    "            df = df.sample(frac=SAMPLE_FRAC, random_state=123456789)\n",
    "            df.reset_index()\n",
    "    return df\n",
    "\n",
    "def get_stopwords():\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stopwords = []\n",
    "        for l in f:\n",
    "            if l not in stopwords:\n",
    "                stopwords.append(l.replace('\\n',''))\n",
    "    return stopwords\n",
    "\n",
    "def get_hierarchy_ingreds():\n",
    "    with open('replace_words.json') as f:\n",
    "        replacements = json.load(f)\n",
    "        for each in replacements.keys():\n",
    "            replacements[each] = ast.literal_eval(replacements[each])\n",
    "    return replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility Functions\n",
    "\"\"\"\n",
    "\n",
    "def stem_ingreds(row: pd.Series, ingred_idx: int)->None:\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in INGREDIENT_TO_REPLACE.split(' ')]\n",
    "    s = ''\n",
    "    for word in words:\n",
    "        s += word\n",
    "    sentence = row['ingredients_list'][ingred_idx]\n",
    "    words = [ps.stem(word) for word in sentence.split(' ')]\n",
    "    s = ''\n",
    "    for word in words:\n",
    "        s += word    \n",
    "    row['ingredients_list'][ingred_idx] = s\n",
    "    row['ingredients_list'][ingred_idx] = row['ingredients_list'][ingred_idx].replace(' ', '')\n",
    "\n",
    "\n",
    "def decompose_hierarchy(row: pd.Series, replacements: dict) -> None: \n",
    "    idx = 0\n",
    "    idx_to_remove = []\n",
    "    while idx<len(row['ingredients_list']):\n",
    "        if row['ingredients_list'][idx] in replacements.keys():\n",
    "            #NEED TO DO THIS. SIMPLE GET THE VALUES IN replacements. CAN'T APPEND IN LOOP, BUT YOU COULD DO A WHILE LOOP\n",
    "            idx_to_remove.append(idx)\n",
    "            if LOAD_TEST_SET:                \n",
    "                if all(item.lower() in replacements[row['ingredients_list'][idx]] \\\n",
    "                    for item in INGREDIENT_TO_REPLACE.lower().split()):\n",
    "                    row['ingredients_list'].append(INGREDIENT_TO_REPLACE)\n",
    "            else:\n",
    "                row['ingredients_list'].extend(replacements[row['ingredients_list'][idx]])             \n",
    "        idx+=1\n",
    "    if REMOVE_REPLACE:\n",
    "        for i in reversed(idx_to_remove):\n",
    "            row['ingredients_list'].pop(i)\n",
    "\n",
    "def remove_stopwords(row:pd.Series,ingred_idx:int, stopwords: list):\n",
    "    stop_row_li = np.array(row['ingredients_list'][ingred_idx].split(' '))\n",
    "    for stopword in stopwords:\n",
    "            stop_loc = np.where(stop_row_li ==stopword)\n",
    "            if len(stop_loc[0])>0:\n",
    "                stop_row_li= np.delete(stop_row_li,stop_loc)\n",
    "                str_ = ''\n",
    "                for stringify_ in stop_row_li:\n",
    "                    str_ += ' ' + stringify_\n",
    "                row['ingredients_list'][ingred_idx] = str_\n",
    "\n",
    "def is_sparse(row: pd.Series) -> bool:\n",
    "    if row['brand'] == math.nan:\n",
    "        return True\n",
    "\n",
    "    len_ing = len(row['ingredients_list'])\n",
    "    if len_ing==0:\n",
    "        return True\n",
    "    elif len_ing == 1 and row['ingredients_list'][0].lower() in ['nan', 'none', 'na']:\n",
    "        return True\n",
    "\n",
    "def combine_all_labels(row: pd.Series, ingred_idx: int)->None:\n",
    "    each = row['ingredients_list'][ingred_idx]\n",
    "    if all(item in each.lower() for item in INGREDIENT_TO_REPLACE.lower().split()):       \n",
    "        row['ingredients_list'][ingred_idx] = INGREDIENT_TO_REPLACE.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jwfos\\AppData\\Local\\Temp\\ipykernel_5512\\217381534.py:8: DtypeWarning: Columns (45,46,47,48,49,50,51,52,53,54,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(test_set_path)\n",
      "4000it [00:00, 4015.83it/s]\n",
      "3712it [00:44, 84.31it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>price_max</th>\n",
       "      <th>price_min</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BANQUET Frozen Chicken Breast Patties Made Wit...</td>\n",
       "      <td>Banquet</td>\n",
       "      <td>['Grocery', 'Grocery &amp; Gourmet Food', 'Breast ...</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.88</td>\n",
       "      <td>['water', 'sodium bicarbonate', 'yeast', 'corn...</td>\n",
       "      <td>[water, sodiumbicarbon, yeast, cornstarch, whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>COLMANS Original English Mustard</td>\n",
       "      <td>Colman's</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'English Mustard', ...</td>\n",
       "      <td>19.73</td>\n",
       "      <td>4.49</td>\n",
       "      <td>['water', 'mustard flour', 'sugar', 'salt', 'w...</td>\n",
       "      <td>[water, mustardflour, sugar, salt, wheatflour,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Against The Grain Three Cheese Pizza</td>\n",
       "      <td>Against The Grain</td>\n",
       "      <td>['Pizza', 'Grocery &amp; Gourmet Food', 'Frozen']</td>\n",
       "      <td>100.32</td>\n",
       "      <td>100.32</td>\n",
       "      <td>['crust tapioca starch', 'milk', 'eggs', 'cano...</td>\n",
       "      <td>[crusttapiocastarch, milk, egg, canolaoil, che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NESTLE COFFEE-MATE Coffee Creamer</td>\n",
       "      <td>Nestle Coffee Mate</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Dairy', ' Cheese &amp;...</td>\n",
       "      <td>22.72</td>\n",
       "      <td>17.48</td>\n",
       "      <td>['water', 'sugar', 'coconut oil', 'sodium case...</td>\n",
       "      <td>[water, sugar, coconutoil, sodiumcasein, dipot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Planters Peanuts</td>\n",
       "      <td>Planters</td>\n",
       "      <td>['Grocery', 'Cooking &amp; Baking', 'Nuts &amp; Trail ...</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.95</td>\n",
       "      <td>['peanut andor cottonseed oil', 'peanuts', 'al...</td>\n",
       "      <td>[peanutcottonseoil, peanut, almond, cashew, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>3994</td>\n",
       "      <td>MORTON Table Salt</td>\n",
       "      <td>Morton</td>\n",
       "      <td>['Plumbing', 'Water Filtration and Water Softe...</td>\n",
       "      <td>27.99</td>\n",
       "      <td>27.99</td>\n",
       "      <td>['salt', 'sodium silicoaluminate']</td>\n",
       "      <td>[salt, sodiumsilicoalumin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>3995</td>\n",
       "      <td>Birds Eye Voila! Family Size Garlic Chicken</td>\n",
       "      <td>Birds Eye</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Poultry', 'Food &amp; ...</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.96</td>\n",
       "      <td>['water', 'corn', 'dehydrated onion', 'ribofla...</td>\n",
       "      <td>[water, corn, onion, riboflavin, folicacid, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>3996</td>\n",
       "      <td>Kar's Nuts Sunflower Kernels Snacks - Bulk Pac...</td>\n",
       "      <td>Kar's</td>\n",
       "      <td>['Cooking &amp; Baking', 'Grocery &amp; Gourmet Food',...</td>\n",
       "      <td>51.15</td>\n",
       "      <td>36.78</td>\n",
       "      <td>['sunflower kernels sunflower kernels', 'peanu...</td>\n",
       "      <td>[sunflowkernelsunflowkernel, peanutsunflowoil,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>3998</td>\n",
       "      <td>Nature Valley Protein Chewy Bars, Peanut Butte...</td>\n",
       "      <td>Nature Valley</td>\n",
       "      <td>['Household &amp; Grocery', 'Food &amp; Snacks', 'Bars']</td>\n",
       "      <td>8.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>['roasted peanuts', 'vegetable glycerin', 'pal...</td>\n",
       "      <td>[roastpeanut, vegetglycerin, palm, chocolliquo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>3999</td>\n",
       "      <td>Kellogg's Special K</td>\n",
       "      <td>Special K</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Powdered Drink Mix...</td>\n",
       "      <td>20.97</td>\n",
       "      <td>16.68</td>\n",
       "      <td>['water', 'nonfat milk', 'soy protein isolate'...</td>\n",
       "      <td>[water, milk, soyprotein, sugar, canolaoil, po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3228 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               name   \n",
       "0         0  BANQUET Frozen Chicken Breast Patties Made Wit...  \\\n",
       "1         1                   COLMANS Original English Mustard   \n",
       "2         2               Against The Grain Three Cheese Pizza   \n",
       "3         4                  NESTLE COFFEE-MATE Coffee Creamer   \n",
       "4         5                                   Planters Peanuts   \n",
       "...     ...                                                ...   \n",
       "3223   3994                                  MORTON Table Salt   \n",
       "3224   3995        Birds Eye Voila! Family Size Garlic Chicken   \n",
       "3225   3996  Kar's Nuts Sunflower Kernels Snacks - Bulk Pac...   \n",
       "3226   3998  Nature Valley Protein Chewy Bars, Peanut Butte...   \n",
       "3227   3999                                Kellogg's Special K   \n",
       "\n",
       "                   brand                                         categories   \n",
       "0                Banquet  ['Grocery', 'Grocery & Gourmet Food', 'Breast ...  \\\n",
       "1               Colman's  ['Grocery & Gourmet Food', 'English Mustard', ...   \n",
       "2      Against The Grain      ['Pizza', 'Grocery & Gourmet Food', 'Frozen']   \n",
       "3     Nestle Coffee Mate  ['Grocery & Gourmet Food', 'Dairy', ' Cheese &...   \n",
       "4               Planters  ['Grocery', 'Cooking & Baking', 'Nuts & Trail ...   \n",
       "...                  ...                                                ...   \n",
       "3223              Morton  ['Plumbing', 'Water Filtration and Water Softe...   \n",
       "3224           Birds Eye  ['Grocery & Gourmet Food', 'Poultry', 'Food & ...   \n",
       "3225               Kar's  ['Cooking & Baking', 'Grocery & Gourmet Food',...   \n",
       "3226       Nature Valley   ['Household & Grocery', 'Food & Snacks', 'Bars']   \n",
       "3227           Special K  ['Grocery & Gourmet Food', 'Powdered Drink Mix...   \n",
       "\n",
       "      price_max  price_min                                        ingredients   \n",
       "0          5.39       3.88  ['water', 'sodium bicarbonate', 'yeast', 'corn...  \\\n",
       "1         19.73       4.49  ['water', 'mustard flour', 'sugar', 'salt', 'w...   \n",
       "2        100.32     100.32  ['crust tapioca starch', 'milk', 'eggs', 'cano...   \n",
       "3         22.72      17.48  ['water', 'sugar', 'coconut oil', 'sodium case...   \n",
       "4          9.99       9.95  ['peanut andor cottonseed oil', 'peanuts', 'al...   \n",
       "...         ...        ...                                                ...   \n",
       "3223      27.99      27.99                 ['salt', 'sodium silicoaluminate']   \n",
       "3224       7.39       6.96  ['water', 'corn', 'dehydrated onion', 'ribofla...   \n",
       "3225      51.15      36.78  ['sunflower kernels sunflower kernels', 'peanu...   \n",
       "3226       8.99       8.99  ['roasted peanuts', 'vegetable glycerin', 'pal...   \n",
       "3227      20.97      16.68  ['water', 'nonfat milk', 'soy protein isolate'...   \n",
       "\n",
       "                                       ingredients_list  \n",
       "0     [water, sodiumbicarbon, yeast, cornstarch, whe...  \n",
       "1     [water, mustardflour, sugar, salt, wheatflour,...  \n",
       "2     [crusttapiocastarch, milk, egg, canolaoil, che...  \n",
       "3     [water, sugar, coconutoil, sodiumcasein, dipot...  \n",
       "4     [peanutcottonseoil, peanut, almond, cashew, pi...  \n",
       "...                                                 ...  \n",
       "3223                         [salt, sodiumsilicoalumin]  \n",
       "3224  [water, corn, onion, riboflavin, folicacid, ca...  \n",
       "3225  [sunflowkernelsunflowkernel, peanutsunflowoil,...  \n",
       "3226  [roastpeanut, vegetglycerin, palm, chocolliquo...  \n",
       "3227  [water, milk, soyprotein, sugar, canolaoil, po...  \n",
       "\n",
       "[3228 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = get_stopwords()\n",
    "if DECOMPOSE_HIERARCHY:\n",
    "    replacements = get_hierarchy_ingreds()\n",
    "    \n",
    "df = get_data()\n",
    "#Add a list of the ingredients, and concatenate palm oil products to be just palm oil\n",
    "ingredients = [[x.lower() for x in ast.literal_eval(product['ingredients'])] for _,product in df.iterrows()]\n",
    "df['ingredients_list'] = ingredients\n",
    "\n",
    "\n",
    "drop_sparse_idx = []\n",
    "\n",
    "\n",
    "trans = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "for row_idx, row in tqdm(df.iterrows()):\n",
    "    if is_sparse(row):\n",
    "        drop_sparse_idx.append(row_idx)\n",
    "    else:\n",
    "        for idx in range(len(row['ingredients_list'])):\n",
    "            row['ingredients_list'][idx] = row['ingredients_list'][idx].translate(trans) \n",
    "        if DECOMPOSE_HIERARCHY:\n",
    "            decompose_hierarchy(row, replacements)\n",
    "\n",
    "df = df.drop(drop_sparse_idx)\n",
    "drop_sparse_idx = []\n",
    "\n",
    "for row_idx, row in tqdm(df.iterrows()):\n",
    "    pop_li = []\n",
    "    for idx, each in enumerate(row['ingredients_list']):        \n",
    "        remove_stopwords(row,idx, stopwords)\n",
    "        if STEM:\n",
    "            stem_ingreds(row, idx)\n",
    "        if row['ingredients_list'][idx] == '':\n",
    "            pop_li.append(idx)\n",
    "        else:\n",
    "            combine_all_labels(row, idx)\n",
    "    if pop_li:\n",
    "        for each in reversed(pop_li):\n",
    "            row['ingredients_list'].pop(each)\n",
    "        if len(row['ingredients_list']) == 0:\n",
    "            drop_sparse_idx.append(row_idx)\n",
    "\n",
    "df = df.drop(drop_sparse_idx)\n",
    "df = df.drop_duplicates(subset=\"name\")\n",
    "\n",
    "\n",
    "df = df[['name','brand','categories','price_max','price_min','ingredients','ingredients_list']]\n",
    "df = df.dropna().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23079306071871128 1.0\n"
     ]
    }
   ],
   "source": [
    "brand = {}\n",
    "num_palm = 0\n",
    "for idx,row in df.iterrows():\n",
    "    for each in row['ingredients_list']:\n",
    "        if INGREDIENT_TO_REPLACE in each.lower():\n",
    "            #print(\"Name: %s ## Ingredient: %s\" % (row['name'], each))            \n",
    "            num_palm += 1\n",
    "            br = row['brand']\n",
    "            brand[br] = brand[br] + 1 if br in brand.keys() else 1\n",
    "print(num_palm/len(df.index), len(df.index)/len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[[\"name\", \"ingredients\"]]\n",
    "# df = df.drop_duplicates(subset=\"name\")\n",
    "# df = df.reset_index()\n",
    "\n",
    "\n",
    "#unique_ingred = np.unique(sum(df[\"ingredients\"].apply(ast.literal_eval).values.tolist(), []))\n",
    "unique_names = df['name'].unique()\n",
    "unique_brands = df['brand'].unique()\n",
    "#unique_mans = df['manufacturer'].unique()\n",
    "unique_cats = np.unique(sum(df[\"categories\"].apply(ast.literal_eval).values.tolist(), []))\n",
    "unique_names = sorted(unique_names)\n",
    "#unique_ingred = sorted(unique_ingred)\n",
    "\n",
    "unique_ingred = []\n",
    "for x in df['ingredients_list']:\n",
    "    for y in x:\n",
    "        if y not in unique_ingred:\n",
    "            unique_ingred.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector_and_label(product, decision_ingredients):\n",
    "  #Build feature vector of high entropy ingredients. \n",
    "  feature_vector = np.zeros(len(decision_ingredients))\n",
    "  ingredients = product['ingredients_list']#ast.literal_eval(product['ingredients'])\n",
    "  for dec_idx,decision_ingr in enumerate(decision_ingredients):\n",
    "    #Check if ingredients is in feature list, and also make sure to mask the palm oil ingredient\n",
    "    if decision_ingredients[dec_idx] in ingredients and decision_ingredients[dec_idx] != INGREDIENT_TO_REPLACE:      \n",
    "      feature_vector[dec_idx] = 1\n",
    "  #1 = palmy, 0 = non_palmy\n",
    "  label = 1 if INGREDIENT_TO_REPLACE in ingredients else 0\n",
    "\n",
    "  return (feature_vector, label)\n",
    "\n",
    "def get_cat_and_price(product):\n",
    "  #get category data\n",
    "  cat_data = ast.literal_eval(product['categories'])\n",
    "  #Get mean price\n",
    "  price_data = (product['price_min']+product['price_max']) / 2\n",
    "  return {'cat':cat_data, 'price':price_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_matrix_from_features(feature_mat):\n",
    "    adj_mat = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        for i in elements:\n",
    "            for j in elements:\n",
    "                adj_mat[i,j] += 1 if i!=j else 0 \n",
    "    return adj_mat\n",
    "\n",
    "def get_adj_matrix_from_extra_features(feature_mat):\n",
    "    adj_mat = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    shared_features = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        for i in elements:\n",
    "            for j in elements:\n",
    "                adj_mat[i,j] += 1 if i!=j else 0\n",
    "                shared_features[i,j] = idx\n",
    "    return (adj_mat, shared_features)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_matrix():\n",
    "    feature_mat = np.zeros((len(unique_names),len(unique_ingred)),dtype=int)\n",
    "    for idx, row in df.iterrows():\n",
    "        ingr = row['ingredients_list']\n",
    "        for ing_idx,each in enumerate(unique_ingred):\n",
    "            feature_mat[idx,ing_idx] = 1 if each in ingr else 0\n",
    "    return feature_mat\n",
    "\n",
    "def get_feature_coocurrance():    \n",
    "    feature_mat = dict()\n",
    "    for each in range(len(unique_ingred)):\n",
    "        feature_mat[each] = np.zeros((len(unique_ingred)),dtype=int)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ingr = row['ingredients_list']#ast.literal_eval(row['ingredients'])\n",
    "        for ing_idx,each in enumerate(unique_ingred):\n",
    "            if each in ingr:\n",
    "                for ing_idx2,each2 in enumerate(unique_ingred):\n",
    "                    if each2 in ingr:\n",
    "                        #Minus the value along the diagonal. Prevents inteference with the coocurrance but also allows us to retrieve magnitude later\n",
    "                        feature_mat[ing_idx][ing_idx2] += 1 if ing_idx!=ing_idx2 else -1\n",
    "    return feature_mat\n",
    "\n",
    "def get_information_entropy(cooccurance_matrix, palm_oil_idx, num_palm_oil, num_total_products):\n",
    "    #Intuition: use information entropy to calculate how much information each other ingredient can give us on the presence of palm oil\n",
    "    # - (P(palm_oil_presence|x) log(P(palm_oil_presence|x))) + (P(¬palm_oil_presence|x) log(P(¬palm_oil_presence|x)))\n",
    "    def calculate_entropy(palm_presence, n):\n",
    "        if n != 0:\n",
    "            p1 = palm_presence/n\n",
    "        else:\n",
    "            p1 = 0\n",
    "        p2 = 1-p1\n",
    "        sum1 = 0 if p1==0 else (p1*math.log2(p1))\n",
    "        sum2 = 0 if p2==0 else (p2*math.log2(p2))\n",
    "        return - sum1 + sum2\n",
    "    \n",
    "    def calcalate_conditional_entropy(palm_presence, n, p_n):\n",
    "        if p_n < 0:\n",
    "            print(p_n)\n",
    "            exit()\n",
    "        if n!= 0:\n",
    "            p1 = (palm_presence/n)*p_n\n",
    "        else:\n",
    "            p1 = 0\n",
    "        assert p1 >=0\n",
    "        assert p_n >=0\n",
    "        sum1 = 1 if (p1<=0 or p_n<=0) else (1 + (p1*math.log2(1+(p1/p_n))))\n",
    "        if sum1 <= 0:\n",
    "            print(p1, p_n, palm_presence, n)\n",
    "        return sum1\n",
    "\n",
    "    n = len(unique_ingred)\n",
    "    entropies = np.empty(n) \n",
    "    for idx,each in enumerate(cooccurance_matrix[palm_oil_idx]):\n",
    "        if idx == palm_oil_idx:\n",
    "            entropies[idx] == 100\n",
    "        else:\n",
    "            n_occ = -cooccurance_matrix[idx][idx]\n",
    "            assert n_occ >= each, (unique_ingred[idx])\n",
    "            entropies[idx] = (calcalate_conditional_entropy(each, n_occ, n_occ/num_total_products) + \n",
    "                                calcalate_conditional_entropy(each, num_total_products-n_occ, (num_total_products-n_occ)/num_total_products) + \n",
    "                                calcalate_conditional_entropy(n_occ-each, n_occ, n_occ/num_total_products) + \n",
    "                                calcalate_conditional_entropy(n_occ-each, num_total_products-n_occ, (num_total_products-n_occ)/num_total_products)\n",
    "                                )\n",
    "    return entropies\n",
    "\n",
    "def frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies):\n",
    "  frequency_weighted_conditional_entropies = []\n",
    "  total_products = len(df.index)\n",
    "  fs = []\n",
    "  min_frequency = 0.00\n",
    "  for idx in ordered_entropy:\n",
    "      #Minus as you've made the diagonal minus for other reasons      \n",
    "      if cooccurance_matrix[idx][idx]!=0:\n",
    "          frequency = (-cooccurance_matrix[idx][idx])/total_products\n",
    "          fs.append(frequency)\n",
    "          if frequency >= min_frequency:\n",
    "              occurance = np.log(1/frequency)\n",
    "              if occurance < 0:\n",
    "                  print(\"yes\")\n",
    "              \n",
    "              frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "          else:\n",
    "              frequency_weighted_conditional_entropies.append(1.0)\n",
    "  return frequency_weighted_conditional_entropies\n",
    "\n",
    "def apply_entropy_mask(feature_matrix,top_n=2000, frequency_weighted=True):\n",
    "  cooccurance_matrix = get_feature_coocurrance()\n",
    "  #Print the highest co-occuring ingredients\n",
    "  for idx,each in enumerate(unique_ingred):\n",
    "      if (INGREDIENT_TO_REPLACE in each.lower()):\n",
    "          palm_idx = idx\n",
    "          palm_vector = cooccurance_matrix[idx]\n",
    "          sorted_indicies = sorted(range(len(palm_vector)), key=lambda x: palm_vector[x], reverse=True)\n",
    "          print(\"MOST CO-OCCURANCES: \",[(unique_ingred[x],palm_vector[x]) for x in sorted_indicies[:10]])\n",
    "          break\n",
    "  entropies = get_information_entropy(cooccurance_matrix, palm_idx, num_palm,len(df.index))\n",
    "  ordered_entropy = sorted(range(len(entropies)), key=lambda x: entropies[x], reverse=False)\n",
    "  \n",
    "  if frequency_weighted:\n",
    "    fwce = frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies)\n",
    "    fwce_ordered_entropy = sorted(range(len(entropies)), key=lambda x: fwce[x], reverse=False)\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in fwce_ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, fwce_ordered_entropy, unique_ingred_\n",
    "  else:\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, ordered_entropy, unique_ingred_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jwfos\\AppData\\Local\\Temp\\ipykernel_5512\\973985669.py:66: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:233.)\n",
      "  feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATRIX Shape:  torch.Size([3228, 7000])\n",
      "Data(x=[3228, 7000], edge_index=[2, 71508], y=[3228], edge_label=[15890], edge_label_index=[2, 15890]) Data(x=[3228, 7000], edge_index=[2, 71508], y=[3228], edge_label_index=[2, 15890], edge_label=[15890]) tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') tensor([0, 0, 0], device='cuda:0') tensor([0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_balanced_dataset(df):\n",
    "  if 'level_0' in df.columns:\n",
    "    df = df.drop('level_0', axis=1)\n",
    "  df = df.reset_index()\n",
    "  palmy_prods = []\n",
    "  for idx,row in df.iterrows():\n",
    "    for each in row['ingredients_list']:    \n",
    "      if (INGREDIENT_TO_REPLACE in each):\n",
    "        palmy_prods.append(row)\n",
    "\n",
    "  palmy_df = pd.DataFrame(palmy_prods)\n",
    "  #Sample the same amount of non-palmy products\n",
    "\n",
    "  df = df.drop(list(palmy_df.index))\n",
    "  palmy_df = palmy_df.drop_duplicates(subset='index')\n",
    "  sample_len = len(palmy_df.index)\n",
    "  non_palmy_df = df.sample(sample_len)\n",
    "  assert len(palmy_df.index) == len(non_palmy_df.index)\n",
    "  frame = pd.concat([palmy_df, non_palmy_df]).drop('level_0', axis=1)\n",
    "  frame = frame.reset_index().drop('level_0', axis=1)\n",
    "  return frame\n",
    "\n",
    "\n",
    "# else:\n",
    "#   df = get_balanced_dataset(df)\n",
    "\n",
    "\n",
    "\n",
    "if LOAD_TEST_SET:\n",
    "  with open(f'{INGREDIENT_TO_REPLACE}_link_test_set_unique_ingreds', 'rb') as f:\n",
    "    unique_ingred = pickle.load(f)\n",
    "    print(len(unique_ingred))\n",
    "else:\n",
    "  #df = get_balanced_dataset(df)\n",
    "  unique_ingred = []\n",
    "  for x in df['ingredients_list']:\n",
    "      for y in x:\n",
    "          if y not in unique_ingred:\n",
    "              unique_ingred.append(y)\n",
    "  df.to_csv(root_to_standard_data)\n",
    "  with open(f'{INGREDIENT_TO_REPLACE}_link_test_set_unique_ingreds', 'wb') as f:\n",
    "    print(\"Saving Ingreds: \",len(unique_ingred))\n",
    "    pickle.dump(unique_ingred, f)\n",
    "\n",
    "#Don't need ingredients, now that we have the ingredients list column\n",
    "df = df.drop('ingredients', axis=1)\n",
    "#Get Matrix of Features -- Products x Features\n",
    "feature_matrix = []\n",
    "label_vector = []\n",
    "for idx, product in df.iterrows():\n",
    "  feature_vector, label = get_feature_vector_and_label(product, unique_ingred)#get_feature_vector_and_label_with_brands(product, unique_ingred)\n",
    "  feature_matrix.append(feature_vector)\n",
    "  label_vector.append(label)\n",
    "\n",
    "\n",
    "\n",
    "# if MASK:\n",
    "#   feature_matrix,ordered_entropy,unique_ingred=apply_entropy_mask(feature_matrix,int(len(unique_ingred)/2), frequency_weighted=USE_FREQUENCY)\n",
    "#   feature_matrix = []\n",
    "#   label_vector = []\n",
    "#   for idx, product in df.iterrows():\n",
    "#     feature_vector, label = get_feature_vector_and_label(product, unique_ingred)#get_feature_vector_and_label_with_brands(product, unique_ingred)\n",
    "#     feature_matrix.append(feature_vector)\n",
    "#     label_vector.append(label)\n",
    "    \n",
    "feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "print('MATRIX Shape: ',feature_matrix.size())\n",
    "\n",
    "label_vector = torch.tensor(label_vector)\n",
    "\n",
    "#Get The category and price information for each node. Used to define edges\n",
    "node_data_for_edges = np.array([get_cat_and_price(product) for _, product in df.iterrows()])\n",
    "edge_index = []\n",
    "edge_data = []\n",
    "\n",
    "price_range = 0.1 #Prices must be within 20%\n",
    "\n",
    "if not LOAD_TEST_SET:\n",
    "  for from_ in tqdm(range(len(node_data_for_edges))):\n",
    "    row = df.iloc[[from_]]\n",
    "    for r in row['ingredients_list']:\n",
    "      if INGREDIENT_TO_REPLACE in r:\n",
    "        # print(\"EACH: \",each)\n",
    "        # if each==:\n",
    "        #if 'palm oil' in list(df.iloc[[from_]]['ingredients_list']):\n",
    "        for to_ in range(len(node_data_for_edges)):\n",
    "          is_palm = False\n",
    "          for to_row in df.loc[[to_]]['ingredients_list']:\n",
    "            for to_each in to_row:\n",
    "              if to_each == INGREDIENT_TO_REPLACE:\n",
    "                is_palm = True\n",
    "          if not is_palm:\n",
    "            #Check if they are similar price\n",
    "            price_ratio = node_data_for_edges[from_]['price']/node_data_for_edges[to_]['price']\n",
    "            if (price_ratio >= 1-price_range) and (price_ratio <= 1+price_range):\n",
    "              #Check if they are in the same category\n",
    "\n",
    "              for cat_ in node_data_for_edges[from_]['cat']:\n",
    "                if cat_ in node_data_for_edges[to_]['cat']:\n",
    "                  edge_index.append([from_, to_])\n",
    "                  edge_data.append([cat_, price_ratio])\n",
    "                  #TODO IMPORTANT: WILL ONLY EVER USE 1 CATEGORY HERE. EVEN IF MULTIPLE ARE SHARED\n",
    "                  break\n",
    "          #break\n",
    "else:\n",
    "  edge_index = [[1,2],[0,1]] # Random edges to be replaced in a moment. Satisfies edge split function\n",
    "\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "#edge_data = torch.tensor(edge_data)\n",
    "\n",
    "num_test = 0.1\n",
    "transform = T.Compose([\n",
    "  T.ToDevice(device),\n",
    "  T.RandomLinkSplit(num_val=0.1,num_test=num_test,is_undirected=False, add_negative_train_samples=False)  \n",
    "])\n",
    "\n",
    "\n",
    "if not LOAD_TEST_SET:\n",
    "  data, val_data, test_data = transform(Data(x=feature_matrix, y=label_vector,edge_index=edge_index))\n",
    "  if MAKE_TEST_SET:  \n",
    "    with open(f'{INGREDIENT_TO_REPLACE}_test_data_object', 'wb') as f:\n",
    "      pickle.dump(test_data, f)\n",
    "      print(data)\n",
    "      print(data.edge_index[:,:10])\n",
    "else:\n",
    "  with open(f'{INGREDIENT_TO_REPLACE}_test_data_object', 'rb') as f:\n",
    "    load_data = pickle.load(f)\n",
    "    test_data = Data(x=feature_matrix, y=label_vector,edge_index=load_data.edge_index)\n",
    "    test_data.edge_label_index = load_data.edge_label_index\n",
    "    test_data.edge_label = load_data.edge_label\n",
    "    test_data = test_data.to(device)    \n",
    "    print(load_data, test_data,load_data.x[:3],test_data.x[:3],load_data.y[:3],test_data.y[:3])\n",
    "#dataset = ProductDataset(data, transform=transform)\n",
    "#dataloader = DataLoader(dataset, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "class SAGENet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "    def get_links(self, z, edge_label_index):\n",
    "        decoded = self.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "        preds = torch.Tensor([0 if x<0.5 else 1 for x in decoded]).to(device)\n",
    "        print(preds.shape)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.0 686 1710\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 71\u001b[0m\n\u001b[0;32m     69\u001b[0m lrs \u001b[39m=\u001b[39m [\u001b[39m0.001\u001b[39m]\n\u001b[0;32m     70\u001b[0m save_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 71\u001b[0m grid_search(hids,outs,lrs,save_path)\n",
      "Cell \u001b[1;32mIn[15], line 49\u001b[0m, in \u001b[0;36mgrid_search\u001b[1;34m(hidden_channels, out_channels, lrs, save_path)\u001b[0m\n\u001b[0;32m     46\u001b[0m test_auc_li \u001b[39m=\u001b[39m []\n\u001b[0;32m     47\u001b[0m test_f1_li \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 49\u001b[0m test_auc, test_acc,test_f1 \u001b[39m=\u001b[39m test()\n\u001b[0;32m     50\u001b[0m test_acc_li\u001b[39m.\u001b[39mappend(test_acc)\n\u001b[0;32m     51\u001b[0m test_auc_li\u001b[39m.\u001b[39mappend(test_auc)\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m, in \u001b[0;36mgrid_search.<locals>.test\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m pred,label,source,target \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(preds,el, edge_label_index\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m], edge_label_index\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m1\u001b[39m]):\n\u001b[0;32m     19\u001b[0m   row \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mSource\u001b[39m\u001b[39m'\u001b[39m: df\u001b[39m.\u001b[39miloc[source][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mSource Ingredients\u001b[39m\u001b[39m'\u001b[39m: df\u001b[39m.\u001b[39miloc[source][\u001b[39m'\u001b[39m\u001b[39mingredients_list\u001b[39m\u001b[39m'\u001b[39m],\\\n\u001b[0;32m     20\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mTarget\u001b[39m\u001b[39m'\u001b[39m: df\u001b[39m.\u001b[39miloc[target][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m'\u001b[39m\u001b[39mTarget Ingredients\u001b[39m\u001b[39m'\u001b[39m: df\u001b[39m.\u001b[39miloc[target][\u001b[39m'\u001b[39m\u001b[39mingredients_list\u001b[39m\u001b[39m'\u001b[39m],\\\n\u001b[0;32m     21\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mPrediction\u001b[39m\u001b[39m'\u001b[39m: pred, \u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m: label}\n\u001b[1;32m---> 22\u001b[0m   predictions \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat([predictions, pd\u001b[39m.\u001b[39;49mSeries(row)])\n\u001b[0;32m     24\u001b[0m f1 \u001b[39m=\u001b[39m f1_score(edge_label\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), preds)\n\u001b[0;32m     25\u001b[0m predictions\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39mpalmoil_link_predictions\u001b[39m\u001b[39m'\u001b[39m)  \n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:385\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    370\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    372\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    373\u001b[0m     objs,\n\u001b[0;32m    374\u001b[0m     axis\u001b[39m=\u001b[39maxis,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    382\u001b[0m     sort\u001b[39m=\u001b[39msort,\n\u001b[0;32m    383\u001b[0m )\n\u001b[1;32m--> 385\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mget_result()\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:616\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    612\u001b[0m             indexers[ax] \u001b[39m=\u001b[39m obj_labels\u001b[39m.\u001b[39mget_indexer(new_labels)\n\u001b[0;32m    614\u001b[0m     mgrs_indexers\u001b[39m.\u001b[39mappend((obj\u001b[39m.\u001b[39m_mgr, indexers))\n\u001b[1;32m--> 616\u001b[0m new_data \u001b[39m=\u001b[39m concatenate_managers(\n\u001b[0;32m    617\u001b[0m     mgrs_indexers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnew_axes, concat_axis\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbm_axis, copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy\n\u001b[0;32m    618\u001b[0m )\n\u001b[0;32m    619\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    620\u001b[0m     new_data\u001b[39m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\internals\\concat.py:241\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m    239\u001b[0m     fastpath \u001b[39m=\u001b[39m blk\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m values\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 241\u001b[0m     values \u001b[39m=\u001b[39m _concatenate_join_units(join_units, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[0;32m    242\u001b[0m     fastpath \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39mif\u001b[39;00m fastpath:\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\internals\\concat.py:580\u001b[0m, in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, copy)\u001b[0m\n\u001b[0;32m    577\u001b[0m has_none_blocks \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(unit\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m unit \u001b[39min\u001b[39;00m join_units)\n\u001b[0;32m    578\u001b[0m upcasted_na \u001b[39m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[1;32m--> 580\u001b[0m to_concat \u001b[39m=\u001b[39m [\n\u001b[0;32m    581\u001b[0m     ju\u001b[39m.\u001b[39mget_reindexed_values(empty_dtype\u001b[39m=\u001b[39mempty_dtype, upcasted_na\u001b[39m=\u001b[39mupcasted_na)\n\u001b[0;32m    582\u001b[0m     \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units\n\u001b[0;32m    583\u001b[0m ]\n\u001b[0;32m    585\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(to_concat) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    586\u001b[0m     \u001b[39m# Only one block, nothing to concatenate.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     concat_values \u001b[39m=\u001b[39m to_concat[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\internals\\concat.py:581\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    577\u001b[0m has_none_blocks \u001b[39m=\u001b[39m \u001b[39many\u001b[39m(unit\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m unit \u001b[39min\u001b[39;00m join_units)\n\u001b[0;32m    578\u001b[0m upcasted_na \u001b[39m=\u001b[39m _dtype_to_na_value(empty_dtype, has_none_blocks)\n\u001b[0;32m    580\u001b[0m to_concat \u001b[39m=\u001b[39m [\n\u001b[1;32m--> 581\u001b[0m     ju\u001b[39m.\u001b[39;49mget_reindexed_values(empty_dtype\u001b[39m=\u001b[39;49mempty_dtype, upcasted_na\u001b[39m=\u001b[39;49mupcasted_na)\n\u001b[0;32m    582\u001b[0m     \u001b[39mfor\u001b[39;00m ju \u001b[39min\u001b[39;00m join_units\n\u001b[0;32m    583\u001b[0m ]\n\u001b[0;32m    585\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(to_concat) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    586\u001b[0m     \u001b[39m# Only one block, nothing to concatenate.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m     concat_values \u001b[39m=\u001b[39m to_concat[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\internals\\concat.py:498\u001b[0m, in \u001b[0;36mJoinUnit.get_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     fill_value \u001b[39m=\u001b[39m upcasted_na\n\u001b[1;32m--> 498\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_is_valid_na_for(empty_dtype):\n\u001b[0;32m    499\u001b[0m         \u001b[39m# note: always holds when self.block.dtype.kind == \"V\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m         blk_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mdtype\n\u001b[0;32m    502\u001b[0m         \u001b[39mif\u001b[39;00m blk_dtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    503\u001b[0m             \u001b[39m# we want to avoid filling with np.nan if we are\u001b[39;00m\n\u001b[0;32m    504\u001b[0m             \u001b[39m# using None; we already know that we are all\u001b[39;00m\n\u001b[0;32m    505\u001b[0m             \u001b[39m# nulls\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\internals\\concat.py:441\u001b[0m, in \u001b[0;36mJoinUnit._is_valid_na_for\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    440\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39;49m(is_valid_na_for_dtype(x, dtype) \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m values\u001b[39m.\u001b[39;49mravel(order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mK\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m    443\u001b[0m na_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mfill_value\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m na_value \u001b[39mis\u001b[39;00m NaT \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, dtype):\n\u001b[0;32m    445\u001b[0m     \u001b[39m# e.g. we are dt64 and other is td64\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[39m# fill_values match but we should not cast self.block.values to dtype\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# TODO: this will need updating if we ever have non-nano dt64/td64\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\internals\\concat.py:441\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mobject\u001b[39m:\n\u001b[0;32m    440\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mvalues\n\u001b[1;32m--> 441\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39m(is_valid_na_for_dtype(x, dtype) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m values\u001b[39m.\u001b[39mravel(order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mK\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    443\u001b[0m na_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock\u001b[39m.\u001b[39mfill_value\n\u001b[0;32m    444\u001b[0m \u001b[39mif\u001b[39;00m na_value \u001b[39mis\u001b[39;00m NaT \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_dtype_equal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, dtype):\n\u001b[0;32m    445\u001b[0m     \u001b[39m# e.g. we are dt64 and other is td64\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[39m# fill_values match but we should not cast self.block.values to dtype\u001b[39;00m\n\u001b[0;32m    447\u001b[0m     \u001b[39m# TODO: this will need updating if we ever have non-nano dt64/td64\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:692\u001b[0m, in \u001b[0;36mis_valid_na_for_dtype\u001b[1;34m(obj, dtype)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_valid_na_for_dtype\u001b[39m(obj, dtype: DtypeObj) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    680\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    681\u001b[0m \u001b[39m    isna check that excludes incompatible dtypes\u001b[39;00m\n\u001b[0;32m    682\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39m    bool\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lib\u001b[39m.\u001b[39mis_scalar(obj) \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m isna(obj):\n\u001b[0;32m    693\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    694\u001b[0m     \u001b[39melif\u001b[39;00m dtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Jwfos\\miniconda3\\envs\\phd\\lib\\site-packages\\pandas\\core\\dtypes\\missing.py:183\u001b[0m, in \u001b[0;36misna\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39misna\u001b[39m(obj: \u001b[39mobject\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m \u001b[39m|\u001b[39m npt\u001b[39m.\u001b[39mNDArray[np\u001b[39m.\u001b[39mbool_] \u001b[39m|\u001b[39m NDFrame:\n\u001b[0;32m    107\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[39m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m     \u001b[39mreturn\u001b[39;00m _isna(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if LOAD_TEST_SET:\n",
    "  def grid_search(hidden_channels:list, out_channels:list,lrs:list,save_path:str):\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "          predictions = pd.DataFrame(columns=['Source', 'Source Ingredients', 'Target', 'Target Ingredients', 'Prediction', 'Label'])\n",
    "          model.eval()\n",
    "          z = model.encode(test_data.x, test_data.edge_index)\n",
    "          neg_edge_index = negative_sampling(edge_index=test_data.edge_index, num_nodes=len(test_data.x), num_neg_samples=test_data.edge_label_index.size(1), method=\"sparse\")\n",
    "          edge_label_index = torch.cat([test_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "          edge_label = torch.cat([test_data.edge_label, test_data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "          out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "          \n",
    "          preds = out.cpu().numpy()\n",
    "          preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "          el = edge_label.cpu().numpy()\n",
    "          acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "          print(preds[0], el[0], edge_label_index.cpu().numpy()[0,0],  edge_label_index.cpu().numpy()[1,0])\n",
    "          for pred,label,source,target in zip(preds,el, edge_label_index.cpu().numpy()[0], edge_label_index.cpu().numpy()[1]):\n",
    "            row = {'Source': df.iloc[source]['name'], 'Source Ingredients': df.iloc[source]['ingredients_list'],\\\n",
    "                  'Target': df.iloc[target]['name'],'Target Ingredients': df.iloc[target]['ingredients_list'],\\\n",
    "                        'Prediction': pred, 'Label': label}\n",
    "            predictions = pd.concat([predictions, pd.Series(row)])\n",
    "            \n",
    "          f1 = f1_score(edge_label.cpu().numpy(), preds)\n",
    "          predictions.to_csv('palmoil_link_predictions')  \n",
    "          #print(len(np.where(o>=0.5)[0]), len(np.where(o<=0.5)[0]), len(o))\n",
    "          return roc_auc_score(edge_label.cpu().numpy(), out.cpu().numpy()), acc, f1\n",
    "    \n",
    "\n",
    "    num_features = len(test_data.x[0])\n",
    "    data_dict = {}\n",
    "    end_str = INGREDIENT_TO_REPLACE\n",
    "    data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{}, 'Validation AUC':{}, 'Validation F1': {}, 'Test F1': {}}\n",
    "    for hidden_chans in hidden_channels:\n",
    "      for out_chans in out_channels:\n",
    "        for lr in lrs:\n",
    "          for i in range(25):\n",
    "            model = SAGENet(num_features, hidden_chans, out_chans).to(device)\n",
    "            model.load_state_dict(torch.load(f'{gnn_path}_{i}'))\n",
    "            model.eval()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            max_acc = 0.0\n",
    "            max_auc = 0.0\n",
    "            test_acc_li = []\n",
    "            test_auc_li = []\n",
    "            test_f1_li = []\n",
    "\n",
    "            test_auc, test_acc,test_f1 = test()\n",
    "            test_acc_li.append(test_acc)\n",
    "            test_auc_li.append(test_auc)\n",
    "            test_f1_li.append(test_f1)\n",
    "\n",
    "              \n",
    "              # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              #       f'Test AUC: {test_auc:.4f}',f'Max AUC: {max_auc:.4f}')\n",
    "\n",
    "            print(f'{i} -- Hids: {hidden_chans} Outs: {out_chans} LR: {lr} Max AUC: {test_auc:.4f} Max Acc: {test_acc: .4f} Max F1: {test_f1: .4f}')\n",
    "\n",
    "            data_dict[end_str]['Test Accuracy'][i] = test_acc_li\n",
    "            data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "            data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "            del(model)\n",
    "          with open(save_path+f'{INGREDIENT_TO_REPLACE}_data_dict_entropy_TESTING', 'wb') as f:\n",
    "            pickle.dump(data_dict, f)\n",
    "      \n",
    "  outs = [128] \n",
    "  hids = [128] \n",
    "  lrs = [0.001]\n",
    "  save_path = ''\n",
    "  grid_search(hids,outs,lrs,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_TEST_SET:\n",
    "  def grid_search(hidden_channels:list, out_channels:list,lrs:list,save_path:str):\n",
    "      def train(optimizer, criterion):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        #palm_oil_mask = np.where(data.y.cpu().numpy()==1)[0]\n",
    "        # data_edge_index = data.edge_index[palm_oil_mask]\n",
    "        # data_edge_label = data.edge_label[palm_oil_mask]\n",
    "        # data_edge_label_index = data.edge_label_index[palm_oil_mask]\n",
    "        \n",
    "        \n",
    "        z=model.encode(data.x, data.edge_index)\n",
    "        #print(data.edge_index.size(),data.edge_label.size(1),len(data.x))\n",
    "        neg_edge_index = negative_sampling(edge_index=data.edge_index, num_nodes=len(data.x), num_neg_samples=data.edge_label_index.size(1), method=\"sparse\")\n",
    "        edge_label_index = torch.cat([data.edge_label_index, neg_edge_index], dim=-1)\n",
    "        edge_label = torch.cat([data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "\n",
    "\n",
    "        out = model.decode(z,edge_label_index).view(-1)\n",
    "        loss = criterion(out,edge_label)\n",
    "        # out = model.decode(z,data.edge_label_index).view(-1)\n",
    "        # loss = criterion(out, data.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = out.cpu().detach().numpy()\n",
    "        preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "        acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "        return loss\n",
    "      \n",
    "      @torch.no_grad()\n",
    "      def test():\n",
    "            model.eval()\n",
    "            z = model.encode(test_data.x, test_data.edge_index)\n",
    "            neg_edge_index = negative_sampling(edge_index=test_data.edge_index, num_nodes=len(test_data.x), num_neg_samples=test_data.edge_label_index.size(1), method=\"sparse\")\n",
    "            edge_label_index = torch.cat([test_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "            edge_label = torch.cat([test_data.edge_label, test_data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "            out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "            \n",
    "            preds = out.cpu().numpy()\n",
    "            preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "            el = edge_label.cpu().numpy()\n",
    "            acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "\n",
    "            f1 = f1_score(edge_label.cpu().numpy(), preds)\n",
    "\n",
    "            #print(len(np.where(o>=0.5)[0]), len(np.where(o<=0.5)[0]), len(o))\n",
    "            return roc_auc_score(edge_label.cpu().numpy(), out.cpu().numpy()), acc, f1\n",
    "      \n",
    "      @torch.no_grad()\n",
    "      def val():\n",
    "            model.eval()\n",
    "            z = model.encode(val_data.x, val_data.edge_index)\n",
    "            neg_edge_index = negative_sampling(edge_index=val_data.edge_index, num_nodes=len(val_data.x), num_neg_samples=val_data.edge_label_index.size(1), method=\"sparse\")\n",
    "            edge_label_index = torch.cat([val_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "            edge_label = torch.cat([val_data.edge_label, val_data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "            out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "            preds = out.cpu().numpy()\n",
    "            preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "            el = edge_label.cpu().numpy()\n",
    "            o =  out.cpu().numpy()\n",
    "\n",
    "            f1 = f1_score(edge_label.cpu().numpy(), preds)\n",
    "            \n",
    "\n",
    "            acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "            #print(len(np.where(o>=0.5)[0]), len(np.where(o<=0.5)[0]), len(o))\n",
    "            return roc_auc_score(edge_label.cpu().numpy(), out.cpu().numpy()), acc,f1\n",
    "\n",
    "      num_features = len(data.x[0])\n",
    "      data_dict = {}\n",
    "      end_str = INGREDIENT_TO_REPLACE\n",
    "      data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{}, 'Validation AUC':{}, 'Validation F1': {}, 'Test F1': {}}\n",
    "      for hidden_chans in hidden_channels:\n",
    "        for out_chans in out_channels:\n",
    "          for lr in lrs:\n",
    "            for i in range(25):\n",
    "              model = SAGENet(num_features, hidden_chans, out_chans).to(device)\n",
    "              optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "              criterion = torch.nn.BCEWithLogitsLoss()\n",
    "              max_acc = 0.0\n",
    "              max_auc = 0.0\n",
    "              acc_li = []\n",
    "              auc_li = []\n",
    "              test_acc_li = []\n",
    "              test_auc_li = []\n",
    "              test_f1_li = []\n",
    "              val_f1_li = []\n",
    "              loss_li = []\n",
    "              stop_at = 0\n",
    "              for epoch in range(1,501):\n",
    "                loss=train(optimizer,criterion)         \n",
    "                loss_li.append(loss.cpu().detach().item())\n",
    "                if (epoch%10)==0 or epoch==1:\n",
    "                  auc,acc,val_f1 = val()\n",
    "                  acc_li.append(acc)\n",
    "                  auc_li.append(auc)\n",
    "                  val_f1_li.append(val_f1)\n",
    "                  test_auc, test_acc,test_f1 = test()\n",
    "                  test_acc_li.append(test_acc)\n",
    "                  test_auc_li.append(test_auc)\n",
    "                  test_f1_li.append(test_f1)\n",
    "                  if acc > max_acc:\n",
    "                    stop_at = epoch\n",
    "                    max_auc = auc \n",
    "                    max_acc = acc \n",
    "                    torch.save(model.state_dict(),f'{gnn_path}_{i}') \n",
    "                # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                #       f'Test AUC: {test_auc:.4f}',f'Max AUC: {max_auc:.4f}')\n",
    "\n",
    "              print(f'{i} -- Hids: {hidden_chans} Outs: {out_chans} LR: {lr} Max AUC: {auc_li[int(stop_at/10)]:.4f} Max Acc: {acc_li[int(stop_at/10)]: .4f} Max F1: {val_f1_li[int(stop_at/10)]}')\n",
    "\n",
    "              data_dict[end_str]['train_loss'][i] = loss_li\n",
    "              data_dict[end_str]['Validation Accuracy'][i] = acc_li\n",
    "              data_dict[end_str]['Validation AUC'][i] = auc_li\n",
    "              data_dict[end_str]['Test Accuracy'][i] = test_acc_li\n",
    "              data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "              data_dict[end_str]['Validation F1'][i] = val_f1_li\n",
    "              data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "              del(model)\n",
    "            with open(save_path+f'{INGREDIENT_TO_REPLACE}_data_dict_entropy_stop_{stop_at}', 'wb') as f:\n",
    "              pickle.dump(data_dict, f)\n",
    "        \n",
    "  outs = [128] \n",
    "  hids = [128] \n",
    "  lrs = [0.001]\n",
    "  save_path = ''\n",
    "\n",
    "  grid_search(hids,outs,lrs,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_graph = to_networkx(final_data)\n",
    "# pos=nx.circular_layout(new_graph)\n",
    "# nx.draw(new_graph, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.y)\n",
    "# new_graph = to_networkx(data)\n",
    "# pos=nx.kamada_kawai_layout(new_graph)\n",
    "\n",
    "# k=20\n",
    "\n",
    "# sampled_nodes = random.sample(new_graph.nodes,k)\n",
    "\n",
    "# sampled_graph = new_graph.subgraph(sampled_nodes)\n",
    "\n",
    "# print(nx.is_bipartite(sampled_graph))\n",
    "\n",
    "# li = list(sampled_nodes)\n",
    "# cm = []\n",
    "# palm_list = []\n",
    "# o_c, b_c = 0,0\n",
    "# for node in sampled_graph:\n",
    "#     if data.y[node] ==1:\n",
    "#         cm.append('orange')\n",
    "#         palm_list.append(node)\n",
    "#         o_c+=1\n",
    "#     else:\n",
    "#         cm.append('blue')\n",
    "#         b_c+=1\n",
    "\n",
    "# pos=nx.bipartite_layout(sampled_graph, nodes=palm_list)\n",
    "# nx.draw(sampled_graph, node_color=cm, pos=pos, edgecolors='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
