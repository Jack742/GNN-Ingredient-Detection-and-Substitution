{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.nn import GCNConv, GATv2Conv, SAGEConv\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_auc_score,precision_score, recall_score, f1_score\n",
    "\n",
    "#www.python-graph-gallery.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNBALANCED_LINK_soy_20_percent_of_data_final_saved_balanced_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "INGREDIENT_TO_REPLACE = 'soy'#'palm oil'#'peanut'#palm oil'#'palm oil'#'soya'#'peanut'#'palm oil''peanut'#'wheat'#'wheat'#'soya'#\n",
    "\n",
    "\n",
    "MASK = False\n",
    "USE_FREQUENCY = False\n",
    "\n",
    "\n",
    "gnn_path = f'{INGREDIENT_TO_REPLACE}_link_pred_gnn'\n",
    "test_set_path = f'{INGREDIENT_TO_REPLACE}_link_test_set.csv'\n",
    "balanced_data_path = f'{INGREDIENT_TO_REPLACE})_link_final_saved_balanced_dataset.csv'\n",
    "\n",
    "LOAD_BALANCED_DATA = False\n",
    "\n",
    "\n",
    "LOAD_TEST_SET = True\n",
    "MAKE_TEST_SET = not LOAD_TEST_SET\n",
    "\n",
    "TEST_SAMPLE_FRAC = 0.08\n",
    "SAMPLE_FRAC = 0.2\n",
    "\n",
    "STEM = True\n",
    "DECOMPOSE_HIERARCHY = True\n",
    "REMOVE_REPLACE = False\n",
    "perc_for_print = str(SAMPLE_FRAC*100).split('.')[0]\n",
    "root_to_standard_data = f'UNBALANCED_LINK_{INGREDIENT_TO_REPLACE}_{perc_for_print}_percent_of_data_final_saved_balanced_dataset.csv'\n",
    "print(root_to_standard_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data()->pd.DataFrame: \n",
    "    \"\"\"\n",
    "    Load data from file, depending on global config parameters set\n",
    "    \"\"\"   \n",
    "    if LOAD_BALANCED_DATA:\n",
    "        df = pd.read_csv(balanced_data_path)\n",
    "    elif LOAD_TEST_SET:\n",
    "        df = pd.read_csv(test_set_path)\n",
    "    else:\n",
    "        files = [\n",
    "            \"outputs/food_1_Y.csv\",\n",
    "            \"outputs/food_2_Y.csv\",\n",
    "            \"outputs/food_3_Y.csv\"\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for file in files:\n",
    "            df = pd.concat([df, pd.read_csv(file)])\n",
    "        \n",
    "        if MAKE_TEST_SET:\n",
    "            #df1 = df.sample(frac=TEST_SAMPLE_FRAC, random_state=123456789)\n",
    "            #df = df.drop(df1.index)\n",
    "            df = df.sample(frac=SAMPLE_FRAC, random_state=123456789)\n",
    "            #df1.reset_index()\n",
    "            df = df.reset_index()\n",
    "            df.to_csv(f'{INGREDIENT_TO_REPLACE}_link_test_set.csv')\n",
    "        else:\n",
    "            df = df.sample(frac=SAMPLE_FRAC, random_state=123456789)\n",
    "            df.reset_index()\n",
    "    return df\n",
    "\n",
    "def get_stopwords():\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stopwords = []\n",
    "        for l in f:\n",
    "            if l not in stopwords:\n",
    "                stopwords.append(l.replace('\\n',''))\n",
    "    return stopwords\n",
    "\n",
    "def get_hierarchy_ingreds():\n",
    "    with open('replace_words.json') as f:\n",
    "        replacements = json.load(f)\n",
    "        for each in replacements.keys():\n",
    "            replacements[each] = ast.literal_eval(replacements[each])\n",
    "    return replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility Functions\n",
    "\"\"\"\n",
    "\n",
    "def stem_ingreds(row: pd.Series, ingred_idx: int)->None:\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in INGREDIENT_TO_REPLACE.split(' ')]\n",
    "    s = ''\n",
    "    for word in words:\n",
    "        s += word\n",
    "    sentence = row['ingredients_list'][ingred_idx]\n",
    "    words = [ps.stem(word) for word in sentence.split(' ')]\n",
    "    s = ''\n",
    "    for word in words:\n",
    "        s += word    \n",
    "    row['ingredients_list'][ingred_idx] = s\n",
    "    row['ingredients_list'][ingred_idx] = row['ingredients_list'][ingred_idx].replace(' ', '')\n",
    "\n",
    "\n",
    "def decompose_hierarchy(row: pd.Series, replacements: dict) -> None: \n",
    "    idx = 0\n",
    "    idx_to_remove = []\n",
    "    while idx<len(row['ingredients_list']):\n",
    "        if row['ingredients_list'][idx] in replacements.keys():\n",
    "            #NEED TO DO THIS. SIMPLE GET THE VALUES IN replacements. CAN'T APPEND IN LOOP, BUT YOU COULD DO A WHILE LOOP\n",
    "            idx_to_remove.append(idx)\n",
    "            if LOAD_TEST_SET:                \n",
    "                if all(item.lower() in replacements[row['ingredients_list'][idx]] \\\n",
    "                    for item in INGREDIENT_TO_REPLACE.lower().split()):\n",
    "                    row['ingredients_list'].append(INGREDIENT_TO_REPLACE)\n",
    "            else:\n",
    "                row['ingredients_list'].extend(replacements[row['ingredients_list'][idx]])             \n",
    "        idx+=1\n",
    "    if REMOVE_REPLACE:\n",
    "        for i in reversed(idx_to_remove):\n",
    "            row['ingredients_list'].pop(i)\n",
    "\n",
    "def remove_stopwords(row:pd.Series,ingred_idx:int, stopwords: list):\n",
    "    stop_row_li = np.array(row['ingredients_list'][ingred_idx].split(' '))\n",
    "    for stopword in stopwords:\n",
    "            stop_loc = np.where(stop_row_li ==stopword)\n",
    "            if len(stop_loc[0])>0:\n",
    "                stop_row_li= np.delete(stop_row_li,stop_loc)\n",
    "                str_ = ''\n",
    "                for stringify_ in stop_row_li:\n",
    "                    str_ += ' ' + stringify_\n",
    "                row['ingredients_list'][ingred_idx] = str_\n",
    "\n",
    "def is_sparse(row: pd.Series) -> bool:\n",
    "    if row['brand'] == math.nan:\n",
    "        return True\n",
    "\n",
    "    len_ing = len(row['ingredients_list'])\n",
    "    if len_ing==0:\n",
    "        return True\n",
    "    elif len_ing == 1 and row['ingredients_list'][0].lower() in ['nan', 'none', 'na']:\n",
    "        return True\n",
    "\n",
    "def combine_all_labels(row: pd.Series, ingred_idx: int)->None:\n",
    "    each = row['ingredients_list'][ingred_idx]\n",
    "    if all(item in each.lower() for item in INGREDIENT_TO_REPLACE.lower().split()):       \n",
    "        row['ingredients_list'][ingred_idx] = INGREDIENT_TO_REPLACE.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3364: DtypeWarning: Columns (45,46,47,48,49,50,51,52,53,54,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n",
      "4000it [00:01, 3334.68it/s]\n",
      "3712it [00:37, 98.29it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>price_max</th>\n",
       "      <th>price_min</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>BANQUET Frozen Chicken Breast Patties Made Wit...</td>\n",
       "      <td>Banquet</td>\n",
       "      <td>['Grocery', 'Grocery &amp; Gourmet Food', 'Breast ...</td>\n",
       "      <td>5.39</td>\n",
       "      <td>3.88</td>\n",
       "      <td>['water', 'sodium bicarbonate', 'yeast', 'corn...</td>\n",
       "      <td>[water, sodiumbicarbon, yeast, cornstarch, whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>COLMANS Original English Mustard</td>\n",
       "      <td>Colman's</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'English Mustard', ...</td>\n",
       "      <td>19.73</td>\n",
       "      <td>4.49</td>\n",
       "      <td>['water', 'mustard flour', 'sugar', 'salt', 'w...</td>\n",
       "      <td>[water, mustardflour, sugar, salt, wheatflour,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Against The Grain Three Cheese Pizza</td>\n",
       "      <td>Against The Grain</td>\n",
       "      <td>['Pizza', 'Grocery &amp; Gourmet Food', 'Frozen']</td>\n",
       "      <td>100.32</td>\n",
       "      <td>100.32</td>\n",
       "      <td>['crust tapioca starch', 'milk', 'eggs', 'cano...</td>\n",
       "      <td>[crusttapiocastarch, milk, egg, canolaoil, che...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NESTLE COFFEE-MATE Coffee Creamer</td>\n",
       "      <td>Nestle Coffee Mate</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Dairy', ' Cheese &amp;...</td>\n",
       "      <td>22.72</td>\n",
       "      <td>17.48</td>\n",
       "      <td>['water', 'sugar', 'coconut oil', 'sodium case...</td>\n",
       "      <td>[water, sugar, coconutoil, sodiumcasein, dipot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Planters Peanuts</td>\n",
       "      <td>Planters</td>\n",
       "      <td>['Grocery', 'Cooking &amp; Baking', 'Nuts &amp; Trail ...</td>\n",
       "      <td>9.99</td>\n",
       "      <td>9.95</td>\n",
       "      <td>['peanut andor cottonseed oil', 'peanuts', 'al...</td>\n",
       "      <td>[peanutcottonseoil, peanut, almond, cashew, pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>3994</td>\n",
       "      <td>MORTON Table Salt</td>\n",
       "      <td>Morton</td>\n",
       "      <td>['Plumbing', 'Water Filtration and Water Softe...</td>\n",
       "      <td>27.99</td>\n",
       "      <td>27.99</td>\n",
       "      <td>['salt', 'sodium silicoaluminate']</td>\n",
       "      <td>[salt, sodiumsilicoalumin]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>3995</td>\n",
       "      <td>Birds Eye Voila! Family Size Garlic Chicken</td>\n",
       "      <td>Birds Eye</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Poultry', 'Food &amp; ...</td>\n",
       "      <td>7.39</td>\n",
       "      <td>6.96</td>\n",
       "      <td>['water', 'corn', 'dehydrated onion', 'ribofla...</td>\n",
       "      <td>[water, corn, onion, riboflavin, folicacid, ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>3996</td>\n",
       "      <td>Kar's Nuts Sunflower Kernels Snacks - Bulk Pac...</td>\n",
       "      <td>Kar's</td>\n",
       "      <td>['Cooking &amp; Baking', 'Grocery &amp; Gourmet Food',...</td>\n",
       "      <td>51.15</td>\n",
       "      <td>36.78</td>\n",
       "      <td>['sunflower kernels sunflower kernels', 'peanu...</td>\n",
       "      <td>[sunflowkernelsunflowkernel, peanutsunflowoil,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>3998</td>\n",
       "      <td>Nature Valley Protein Chewy Bars, Peanut Butte...</td>\n",
       "      <td>Nature Valley</td>\n",
       "      <td>['Household &amp; Grocery', 'Food &amp; Snacks', 'Bars']</td>\n",
       "      <td>8.99</td>\n",
       "      <td>8.99</td>\n",
       "      <td>['roasted peanuts', 'vegetable glycerin', 'pal...</td>\n",
       "      <td>[roastpeanut, vegetglycerin, palm, chocolliquo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>3999</td>\n",
       "      <td>Kellogg's Special K</td>\n",
       "      <td>Special K</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Powdered Drink Mix...</td>\n",
       "      <td>20.97</td>\n",
       "      <td>16.68</td>\n",
       "      <td>['water', 'nonfat milk', 'soy protein isolate'...</td>\n",
       "      <td>[water, milk, soy, sugar, canolaoil, polydextr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3228 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               name  \\\n",
       "0         0  BANQUET Frozen Chicken Breast Patties Made Wit...   \n",
       "1         1                   COLMANS Original English Mustard   \n",
       "2         2               Against The Grain Three Cheese Pizza   \n",
       "3         4                  NESTLE COFFEE-MATE Coffee Creamer   \n",
       "4         5                                   Planters Peanuts   \n",
       "...     ...                                                ...   \n",
       "3223   3994                                  MORTON Table Salt   \n",
       "3224   3995        Birds Eye Voila! Family Size Garlic Chicken   \n",
       "3225   3996  Kar's Nuts Sunflower Kernels Snacks - Bulk Pac...   \n",
       "3226   3998  Nature Valley Protein Chewy Bars, Peanut Butte...   \n",
       "3227   3999                                Kellogg's Special K   \n",
       "\n",
       "                   brand                                         categories  \\\n",
       "0                Banquet  ['Grocery', 'Grocery & Gourmet Food', 'Breast ...   \n",
       "1               Colman's  ['Grocery & Gourmet Food', 'English Mustard', ...   \n",
       "2      Against The Grain      ['Pizza', 'Grocery & Gourmet Food', 'Frozen']   \n",
       "3     Nestle Coffee Mate  ['Grocery & Gourmet Food', 'Dairy', ' Cheese &...   \n",
       "4               Planters  ['Grocery', 'Cooking & Baking', 'Nuts & Trail ...   \n",
       "...                  ...                                                ...   \n",
       "3223              Morton  ['Plumbing', 'Water Filtration and Water Softe...   \n",
       "3224           Birds Eye  ['Grocery & Gourmet Food', 'Poultry', 'Food & ...   \n",
       "3225               Kar's  ['Cooking & Baking', 'Grocery & Gourmet Food',...   \n",
       "3226       Nature Valley   ['Household & Grocery', 'Food & Snacks', 'Bars']   \n",
       "3227           Special K  ['Grocery & Gourmet Food', 'Powdered Drink Mix...   \n",
       "\n",
       "      price_max  price_min                                        ingredients  \\\n",
       "0          5.39       3.88  ['water', 'sodium bicarbonate', 'yeast', 'corn...   \n",
       "1         19.73       4.49  ['water', 'mustard flour', 'sugar', 'salt', 'w...   \n",
       "2        100.32     100.32  ['crust tapioca starch', 'milk', 'eggs', 'cano...   \n",
       "3         22.72      17.48  ['water', 'sugar', 'coconut oil', 'sodium case...   \n",
       "4          9.99       9.95  ['peanut andor cottonseed oil', 'peanuts', 'al...   \n",
       "...         ...        ...                                                ...   \n",
       "3223      27.99      27.99                 ['salt', 'sodium silicoaluminate']   \n",
       "3224       7.39       6.96  ['water', 'corn', 'dehydrated onion', 'ribofla...   \n",
       "3225      51.15      36.78  ['sunflower kernels sunflower kernels', 'peanu...   \n",
       "3226       8.99       8.99  ['roasted peanuts', 'vegetable glycerin', 'pal...   \n",
       "3227      20.97      16.68  ['water', 'nonfat milk', 'soy protein isolate'...   \n",
       "\n",
       "                                       ingredients_list  \n",
       "0     [water, sodiumbicarbon, yeast, cornstarch, whe...  \n",
       "1     [water, mustardflour, sugar, salt, wheatflour,...  \n",
       "2     [crusttapiocastarch, milk, egg, canolaoil, che...  \n",
       "3     [water, sugar, coconutoil, sodiumcasein, dipot...  \n",
       "4     [peanutcottonseoil, peanut, almond, cashew, pi...  \n",
       "...                                                 ...  \n",
       "3223                         [salt, sodiumsilicoalumin]  \n",
       "3224  [water, corn, onion, riboflavin, folicacid, ca...  \n",
       "3225  [sunflowkernelsunflowkernel, peanutsunflowoil,...  \n",
       "3226  [roastpeanut, vegetglycerin, palm, chocolliquo...  \n",
       "3227  [water, milk, soy, sugar, canolaoil, polydextr...  \n",
       "\n",
       "[3228 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = get_stopwords()\n",
    "if DECOMPOSE_HIERARCHY:\n",
    "    replacements = get_hierarchy_ingreds()\n",
    "    \n",
    "df = get_data()\n",
    "#Add a list of the ingredients, and concatenate palm oil products to be just palm oil\n",
    "ingredients = [[x.lower() for x in ast.literal_eval(product['ingredients'])] for _,product in df.iterrows()]\n",
    "df['ingredients_list'] = ingredients\n",
    "\n",
    "\n",
    "drop_sparse_idx = []\n",
    "\n",
    "\n",
    "trans = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "for row_idx, row in tqdm(df.iterrows()):\n",
    "    if is_sparse(row):\n",
    "        drop_sparse_idx.append(row_idx)\n",
    "    else:\n",
    "        for idx in range(len(row['ingredients_list'])):\n",
    "            row['ingredients_list'][idx] = row['ingredients_list'][idx].translate(trans) \n",
    "        if DECOMPOSE_HIERARCHY:\n",
    "            decompose_hierarchy(row, replacements)\n",
    "\n",
    "df = df.drop(drop_sparse_idx)\n",
    "drop_sparse_idx = []\n",
    "\n",
    "for row_idx, row in tqdm(df.iterrows()):\n",
    "    pop_li = []\n",
    "    for idx, each in enumerate(row['ingredients_list']):        \n",
    "        remove_stopwords(row,idx, stopwords)\n",
    "        if STEM:\n",
    "            stem_ingreds(row, idx)\n",
    "        if row['ingredients_list'][idx] == '':\n",
    "            pop_li.append(idx)\n",
    "        else:\n",
    "            combine_all_labels(row, idx)\n",
    "    if pop_li:\n",
    "        for each in reversed(pop_li):\n",
    "            row['ingredients_list'].pop(each)\n",
    "        if len(row['ingredients_list']) == 0:\n",
    "            drop_sparse_idx.append(row_idx)\n",
    "\n",
    "df = df.drop(drop_sparse_idx)\n",
    "df = df.drop_duplicates(subset=\"name\")\n",
    "\n",
    "\n",
    "df = df[['name','brand','categories','price_max','price_min','ingredients','ingredients_list']]\n",
    "df = df.dropna().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7283147459727385 1.0\n"
     ]
    }
   ],
   "source": [
    "brand = {}\n",
    "num_palm = 0\n",
    "for idx,row in df.iterrows():\n",
    "    for each in row['ingredients_list']:\n",
    "        if INGREDIENT_TO_REPLACE in each.lower():\n",
    "            #print(\"Name: %s ## Ingredient: %s\" % (row['name'], each))            \n",
    "            num_palm += 1\n",
    "            br = row['brand']\n",
    "            brand[br] = brand[br] + 1 if br in brand.keys() else 1\n",
    "print(num_palm/len(df.index), len(df.index)/len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[[\"name\", \"ingredients\"]]\n",
    "# df = df.drop_duplicates(subset=\"name\")\n",
    "# df = df.reset_index()\n",
    "\n",
    "\n",
    "#unique_ingred = np.unique(sum(df[\"ingredients\"].apply(ast.literal_eval).values.tolist(), []))\n",
    "unique_names = df['name'].unique()\n",
    "unique_brands = df['brand'].unique()\n",
    "#unique_mans = df['manufacturer'].unique()\n",
    "unique_cats = np.unique(sum(df[\"categories\"].apply(ast.literal_eval).values.tolist(), []))\n",
    "unique_names = sorted(unique_names)\n",
    "#unique_ingred = sorted(unique_ingred)\n",
    "\n",
    "unique_ingred = []\n",
    "for x in df['ingredients_list']:\n",
    "    for y in x:\n",
    "        if y not in unique_ingred:\n",
    "            unique_ingred.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_vector_and_label(product, decision_ingredients):\n",
    "  #Build feature vector of high entropy ingredients. \n",
    "  feature_vector = np.zeros(len(decision_ingredients))\n",
    "  ingredients = product['ingredients_list']#ast.literal_eval(product['ingredients'])\n",
    "  for dec_idx,decision_ingr in enumerate(decision_ingredients):\n",
    "    #Check if ingredients is in feature list, and also make sure to mask the palm oil ingredient\n",
    "    if decision_ingredients[dec_idx] in ingredients and decision_ingredients[dec_idx] != INGREDIENT_TO_REPLACE:      \n",
    "      feature_vector[dec_idx] = 1\n",
    "  #1 = palmy, 0 = non_palmy\n",
    "  label = 1 if INGREDIENT_TO_REPLACE in ingredients else 0\n",
    "\n",
    "  return (feature_vector, label)\n",
    "\n",
    "def get_cat_and_price(product):\n",
    "  #get category data\n",
    "  cat_data = ast.literal_eval(product['categories'])\n",
    "  #Get mean price\n",
    "  price_data = (product['price_min']+product['price_max']) / 2\n",
    "  return {'cat':cat_data, 'price':price_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_matrix_from_features(feature_mat):\n",
    "    adj_mat = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        for i in elements:\n",
    "            for j in elements:\n",
    "                adj_mat[i,j] += 1 if i!=j else 0 \n",
    "    return adj_mat\n",
    "\n",
    "def get_adj_matrix_from_extra_features(feature_mat):\n",
    "    adj_mat = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    shared_features = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        for i in elements:\n",
    "            for j in elements:\n",
    "                adj_mat[i,j] += 1 if i!=j else 0\n",
    "                shared_features[i,j] = idx\n",
    "    return (adj_mat, shared_features)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_matrix():\n",
    "    feature_mat = np.zeros((len(unique_names),len(unique_ingred)),dtype=int)\n",
    "    for idx, row in df.iterrows():\n",
    "        ingr = row['ingredients_list']\n",
    "        for ing_idx,each in enumerate(unique_ingred):\n",
    "            feature_mat[idx,ing_idx] = 1 if each in ingr else 0\n",
    "    return feature_mat\n",
    "\n",
    "def get_feature_coocurrance():    \n",
    "    feature_mat = dict()\n",
    "    for each in range(len(unique_ingred)):\n",
    "        feature_mat[each] = np.zeros((len(unique_ingred)),dtype=int)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ingr = row['ingredients_list']#ast.literal_eval(row['ingredients'])\n",
    "        for ing_idx,each in enumerate(unique_ingred):\n",
    "            if each in ingr:\n",
    "                for ing_idx2,each2 in enumerate(unique_ingred):\n",
    "                    if each2 in ingr:\n",
    "                        #Minus the value along the diagonal. Prevents inteference with the coocurrance but also allows us to retrieve magnitude later\n",
    "                        feature_mat[ing_idx][ing_idx2] += 1 if ing_idx!=ing_idx2 else -1\n",
    "    return feature_mat\n",
    "\n",
    "def get_information_entropy(cooccurance_matrix, palm_oil_idx, num_palm_oil, num_total_products):\n",
    "    #Intuition: use information entropy to calculate how much information each other ingredient can give us on the presence of palm oil\n",
    "    # - (P(palm_oil_presence|x) log(P(palm_oil_presence|x))) + (P(¬palm_oil_presence|x) log(P(¬palm_oil_presence|x)))\n",
    "    def calculate_entropy(palm_presence, n):\n",
    "        if n != 0:\n",
    "            p1 = palm_presence/n\n",
    "        else:\n",
    "            p1 = 0\n",
    "        p2 = 1-p1\n",
    "        sum1 = 0 if p1==0 else (p1*math.log2(p1))\n",
    "        sum2 = 0 if p2==0 else (p2*math.log2(p2))\n",
    "        return - sum1 + sum2\n",
    "    \n",
    "    def calcalate_conditional_entropy(palm_presence, n, p_n):\n",
    "        if p_n < 0:\n",
    "            print(p_n)\n",
    "            exit()\n",
    "        if n!= 0:\n",
    "            p1 = (palm_presence/n)*p_n\n",
    "        else:\n",
    "            p1 = 0\n",
    "        assert p1 >=0\n",
    "        assert p_n >=0\n",
    "        sum1 = 1 if (p1<=0 or p_n<=0) else (1 + (p1*math.log2(1+(p1/p_n))))\n",
    "        if sum1 <= 0:\n",
    "            print(p1, p_n, palm_presence, n)\n",
    "        return sum1\n",
    "\n",
    "    n = len(unique_ingred)\n",
    "    entropies = np.empty(n) \n",
    "    for idx,each in enumerate(cooccurance_matrix[palm_oil_idx]):\n",
    "        if idx == palm_oil_idx:\n",
    "            entropies[idx] == 100\n",
    "        else:\n",
    "            n_occ = -cooccurance_matrix[idx][idx]\n",
    "            assert n_occ >= each, (unique_ingred[idx])\n",
    "            entropies[idx] = (calcalate_conditional_entropy(each, n_occ, n_occ/num_total_products) + \n",
    "                                calcalate_conditional_entropy(each, num_total_products-n_occ, (num_total_products-n_occ)/num_total_products) + \n",
    "                                calcalate_conditional_entropy(n_occ-each, n_occ, n_occ/num_total_products) + \n",
    "                                calcalate_conditional_entropy(n_occ-each, num_total_products-n_occ, (num_total_products-n_occ)/num_total_products)\n",
    "                                )\n",
    "    return entropies\n",
    "\n",
    "def frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies):\n",
    "  frequency_weighted_conditional_entropies = []\n",
    "  total_products = len(df.index)\n",
    "  fs = []\n",
    "  min_frequency = 0.00\n",
    "  for idx in ordered_entropy:\n",
    "      #Minus as you've made the diagonal minus for other reasons      \n",
    "      if cooccurance_matrix[idx][idx]!=0:\n",
    "          frequency = (-cooccurance_matrix[idx][idx])/total_products\n",
    "          fs.append(frequency)\n",
    "          if frequency >= min_frequency:\n",
    "              occurance = np.log(1/frequency)\n",
    "              if occurance < 0:\n",
    "                  print(\"yes\")\n",
    "              \n",
    "              frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "          else:\n",
    "              frequency_weighted_conditional_entropies.append(1.0)\n",
    "  return frequency_weighted_conditional_entropies\n",
    "\n",
    "def apply_entropy_mask(feature_matrix,top_n=2000, frequency_weighted=True):\n",
    "  cooccurance_matrix = get_feature_coocurrance()\n",
    "  #Print the highest co-occuring ingredients\n",
    "  for idx,each in enumerate(unique_ingred):\n",
    "      if (INGREDIENT_TO_REPLACE in each.lower()):\n",
    "          palm_idx = idx\n",
    "          palm_vector = cooccurance_matrix[idx]\n",
    "          sorted_indicies = sorted(range(len(palm_vector)), key=lambda x: palm_vector[x], reverse=True)\n",
    "          print(\"MOST CO-OCCURANCES: \",[(unique_ingred[x],palm_vector[x]) for x in sorted_indicies[:10]])\n",
    "          break\n",
    "  entropies = get_information_entropy(cooccurance_matrix, palm_idx, num_palm,len(df.index))\n",
    "  ordered_entropy = sorted(range(len(entropies)), key=lambda x: entropies[x], reverse=False)\n",
    "  \n",
    "  if frequency_weighted:\n",
    "    fwce = frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies)\n",
    "    fwce_ordered_entropy = sorted(range(len(entropies)), key=lambda x: fwce[x], reverse=False)\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in fwce_ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, fwce_ordered_entropy, unique_ingred_\n",
    "  else:\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, ordered_entropy, unique_ingred_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6740\n",
      "MATRIX Shape:  torch.Size([3228, 6740])\n",
      "Data(x=[3228, 6740], edge_index=[2, 105728], y=[3228], edge_label=[23494], edge_label_index=[2, 23494]) Data(x=[3228, 6740], edge_index=[2, 105728], y=[3228], edge_label_index=[2, 23494], edge_label=[23494]) tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0') tensor([1, 0, 0], device='cuda:0') tensor([1, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def get_balanced_dataset(df):\n",
    "  if 'level_0' in df.columns:\n",
    "    df = df.drop('level_0', axis=1)\n",
    "  df = df.reset_index()\n",
    "  palmy_prods = []\n",
    "  for idx,row in df.iterrows():\n",
    "    for each in row['ingredients_list']:    \n",
    "      if (INGREDIENT_TO_REPLACE in each):\n",
    "        palmy_prods.append(row)\n",
    "\n",
    "  palmy_df = pd.DataFrame(palmy_prods)\n",
    "  #Sample the same amount of non-palmy products\n",
    "\n",
    "  df = df.drop(list(palmy_df.index))\n",
    "  palmy_df = palmy_df.drop_duplicates(subset='index')\n",
    "  sample_len = len(palmy_df.index)\n",
    "  non_palmy_df = df.sample(sample_len)\n",
    "  assert len(palmy_df.index) == len(non_palmy_df.index)\n",
    "  frame = pd.concat([palmy_df, non_palmy_df]).drop('level_0', axis=1)\n",
    "  frame = frame.reset_index().drop('level_0', axis=1)\n",
    "  return frame\n",
    "\n",
    "\n",
    "# else:\n",
    "#   df = get_balanced_dataset(df)\n",
    "\n",
    "\n",
    "\n",
    "if LOAD_TEST_SET:\n",
    "  with open(f'{INGREDIENT_TO_REPLACE}_link_test_set_unique_ingreds', 'rb') as f:\n",
    "    unique_ingred = pickle.load(f)\n",
    "    print(len(unique_ingred))\n",
    "else:\n",
    "  #df = get_balanced_dataset(df)\n",
    "  unique_ingred = []\n",
    "  for x in df['ingredients_list']:\n",
    "      for y in x:\n",
    "          if y not in unique_ingred:\n",
    "              unique_ingred.append(y)\n",
    "  df.to_csv(root_to_standard_data)\n",
    "  with open(f'{INGREDIENT_TO_REPLACE}_link_test_set_unique_ingreds', 'wb') as f:\n",
    "    print(\"Saving Ingreds: \",len(unique_ingred))\n",
    "    pickle.dump(unique_ingred, f)\n",
    "\n",
    "#Don't need ingredients, now that we have the ingredients list column\n",
    "df = df.drop('ingredients', axis=1)\n",
    "#Get Matrix of Features -- Products x Features\n",
    "feature_matrix = []\n",
    "label_vector = []\n",
    "for idx, product in df.iterrows():\n",
    "  feature_vector, label = get_feature_vector_and_label(product, unique_ingred)#get_feature_vector_and_label_with_brands(product, unique_ingred)\n",
    "  feature_matrix.append(feature_vector)\n",
    "  label_vector.append(label)\n",
    "\n",
    "\n",
    "\n",
    "# if MASK:\n",
    "#   feature_matrix,ordered_entropy,unique_ingred=apply_entropy_mask(feature_matrix,int(len(unique_ingred)/2), frequency_weighted=USE_FREQUENCY)\n",
    "#   feature_matrix = []\n",
    "#   label_vector = []\n",
    "#   for idx, product in df.iterrows():\n",
    "#     feature_vector, label = get_feature_vector_and_label(product, unique_ingred)#get_feature_vector_and_label_with_brands(product, unique_ingred)\n",
    "#     feature_matrix.append(feature_vector)\n",
    "#     label_vector.append(label)\n",
    "    \n",
    "feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "print('MATRIX Shape: ',feature_matrix.size())\n",
    "\n",
    "label_vector = torch.tensor(label_vector)\n",
    "\n",
    "#Get The category and price information for each node. Used to define edges\n",
    "node_data_for_edges = np.array([get_cat_and_price(product) for _, product in df.iterrows()])\n",
    "edge_index = []\n",
    "edge_data = []\n",
    "\n",
    "price_range = 0.1 #Prices must be within 20%\n",
    "\n",
    "if not LOAD_TEST_SET:\n",
    "  for from_ in tqdm(range(len(node_data_for_edges))):\n",
    "    row = df.iloc[[from_]]\n",
    "    for r in row['ingredients_list']:\n",
    "      if INGREDIENT_TO_REPLACE in r:\n",
    "        # print(\"EACH: \",each)\n",
    "        # if each==:\n",
    "        #if 'palm oil' in list(df.iloc[[from_]]['ingredients_list']):\n",
    "        for to_ in range(len(node_data_for_edges)):\n",
    "          is_palm = False\n",
    "          for to_row in df.loc[[to_]]['ingredients_list']:\n",
    "            for to_each in to_row:\n",
    "              if to_each == INGREDIENT_TO_REPLACE:\n",
    "                is_palm = True\n",
    "          if not is_palm:\n",
    "            #Check if they are similar price\n",
    "            price_ratio = node_data_for_edges[from_]['price']/node_data_for_edges[to_]['price']\n",
    "            if (price_ratio >= 1-price_range) and (price_ratio <= 1+price_range):\n",
    "              #Check if they are in the same category\n",
    "\n",
    "              for cat_ in node_data_for_edges[from_]['cat']:\n",
    "                if cat_ in node_data_for_edges[to_]['cat']:\n",
    "                  edge_index.append([from_, to_])\n",
    "                  edge_data.append([cat_, price_ratio])\n",
    "                  #TODO IMPORTANT: WILL ONLY EVER USE 1 CATEGORY HERE. EVEN IF MULTIPLE ARE SHARED\n",
    "                  break\n",
    "          #break\n",
    "else:\n",
    "  edge_index = [[1,2],[0,1]] # Random edges to be replaced in a moment. Satisfies edge split function\n",
    "\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "#edge_data = torch.tensor(edge_data)\n",
    "\n",
    "num_test = 0.1\n",
    "transform = T.Compose([\n",
    "  T.ToDevice(device),\n",
    "  T.RandomLinkSplit(num_val=0.1,num_test=num_test,is_undirected=False, add_negative_train_samples=False)  \n",
    "])\n",
    "\n",
    "\n",
    "if not LOAD_TEST_SET:\n",
    "  data, val_data, test_data = transform(Data(x=feature_matrix, y=label_vector,edge_index=edge_index))\n",
    "  if MAKE_TEST_SET:  \n",
    "    with open(f'{INGREDIENT_TO_REPLACE}_test_data_object', 'wb') as f:\n",
    "      pickle.dump(test_data, f)\n",
    "      print(data)\n",
    "      print(data.edge_index[:,:10])\n",
    "else:\n",
    "  with open(f'{INGREDIENT_TO_REPLACE}_test_data_object', 'rb') as f:\n",
    "    load_data = pickle.load(f)\n",
    "    test_data = Data(x=feature_matrix, y=label_vector,edge_index=load_data.edge_index)\n",
    "    test_data.edge_label_index = load_data.edge_label_index\n",
    "    test_data.edge_label = load_data.edge_label\n",
    "    test_data = test_data.to(device)    \n",
    "    print(load_data, test_data,load_data.x[:3],test_data.x[:3],load_data.y[:3],test_data.y[:3])\n",
    "#dataset = ProductDataset(data, transform=transform)\n",
    "#dataloader = DataLoader(dataset, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GATv2Conv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "class SAGENet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_label_index):\n",
    "        return (z[edge_label_index[0]] * z[edge_label_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def decode_all(self, z):\n",
    "        prob_adj = z @ z.t()\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t()\n",
    "\n",
    "    def get_links(self, z, edge_label_index):\n",
    "        decoded = self.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "        preds = torch.Tensor([0 if x<0.5 else 1 for x in decoded]).to(device)\n",
    "        print(preds.shape)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9861 Max Acc:  0.8121 Max F1:  0.7267\n",
      "1 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9857 Max Acc:  0.8072 Max F1:  0.7216\n",
      "2 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9860 Max Acc:  0.8047 Max F1:  0.7190\n",
      "3 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9854 Max Acc:  0.8081 Max F1:  0.7225\n",
      "4 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9852 Max Acc:  0.8098 Max F1:  0.7243\n",
      "5 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9860 Max Acc:  0.8042 Max F1:  0.7185\n",
      "6 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9859 Max Acc:  0.8094 Max F1:  0.7239\n",
      "7 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9862 Max Acc:  0.8143 Max F1:  0.7290\n",
      "8 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9859 Max Acc:  0.8100 Max F1:  0.7245\n",
      "9 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9855 Max Acc:  0.8106 Max F1:  0.7252\n",
      "10 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9860 Max Acc:  0.8095 Max F1:  0.7239\n",
      "11 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9856 Max Acc:  0.8081 Max F1:  0.7225\n",
      "12 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9857 Max Acc:  0.8076 Max F1:  0.7220\n",
      "13 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9859 Max Acc:  0.8088 Max F1:  0.7232\n",
      "14 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9855 Max Acc:  0.8064 Max F1:  0.7207\n",
      "15 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9853 Max Acc:  0.8161 Max F1:  0.7309\n",
      "16 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9862 Max Acc:  0.8121 Max F1:  0.7267\n",
      "17 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9855 Max Acc:  0.8100 Max F1:  0.7245\n",
      "18 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9856 Max Acc:  0.8063 Max F1:  0.7206\n",
      "19 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9862 Max Acc:  0.8139 Max F1:  0.7287\n",
      "20 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9858 Max Acc:  0.8085 Max F1:  0.7229\n",
      "21 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9852 Max Acc:  0.8104 Max F1:  0.7248\n",
      "22 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9853 Max Acc:  0.8080 Max F1:  0.7224\n",
      "23 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9848 Max Acc:  0.8035 Max F1:  0.7176\n",
      "24 -- Hids: 128 Outs: 128 LR: 0.001 Max AUC: 0.9853 Max Acc:  0.8104 Max F1:  0.7249\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TEST_SET:\n",
    "  def grid_search(hidden_channels:list, out_channels:list,lrs:list,save_path:str):\n",
    "    @torch.no_grad()\n",
    "    def test():\n",
    "          model.eval()\n",
    "          z = model.encode(test_data.x, test_data.edge_index)\n",
    "          neg_edge_index = negative_sampling(edge_index=test_data.edge_index, num_nodes=len(test_data.x), num_neg_samples=test_data.edge_label_index.size(1), method=\"sparse\")\n",
    "          edge_label_index = torch.cat([test_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "          edge_label = torch.cat([test_data.edge_label, test_data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "          out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "          \n",
    "          preds = out.cpu().numpy()\n",
    "          preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "          el = edge_label.cpu().numpy()\n",
    "          acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "\n",
    "          f1 = f1_score(edge_label.cpu().numpy(), preds)\n",
    "\n",
    "          #print(len(np.where(o>=0.5)[0]), len(np.where(o<=0.5)[0]), len(o))\n",
    "          return roc_auc_score(edge_label.cpu().numpy(), out.cpu().numpy()), acc, f1\n",
    "    \n",
    "\n",
    "    num_features = len(test_data.x[0])\n",
    "    data_dict = {}\n",
    "    end_str = INGREDIENT_TO_REPLACE\n",
    "    data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{}, 'Validation AUC':{}, 'Validation F1': {}, 'Test F1': {}}\n",
    "    for hidden_chans in hidden_channels:\n",
    "      for out_chans in out_channels:\n",
    "        for lr in lrs:\n",
    "          for i in range(25):\n",
    "            model = SAGENet(num_features, hidden_chans, out_chans).to(device)\n",
    "            model.load_state_dict(torch.load(f'{gnn_path}_{i}'))\n",
    "            model.eval()\n",
    "            optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "            criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            max_acc = 0.0\n",
    "            max_auc = 0.0\n",
    "            test_acc_li = []\n",
    "            test_auc_li = []\n",
    "            test_f1_li = []\n",
    "\n",
    "            test_auc, test_acc,test_f1 = test()\n",
    "            test_acc_li.append(test_acc)\n",
    "            test_auc_li.append(test_auc)\n",
    "            test_f1_li.append(test_f1)\n",
    "\n",
    "              \n",
    "              # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              #       f'Test AUC: {test_auc:.4f}',f'Max AUC: {max_auc:.4f}')\n",
    "\n",
    "            print(f'{i} -- Hids: {hidden_chans} Outs: {out_chans} LR: {lr} Max AUC: {test_auc:.4f} Max Acc: {test_acc: .4f} Max F1: {test_f1: .4f}')\n",
    "\n",
    "            data_dict[end_str]['Test Accuracy'][i] = test_acc_li\n",
    "            data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "            data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "            del(model)\n",
    "          with open(save_path+f'{INGREDIENT_TO_REPLACE}_data_dict_entropy_TESTING', 'wb') as f:\n",
    "            pickle.dump(data_dict, f)\n",
    "      \n",
    "  outs = [128] \n",
    "  hids = [128] \n",
    "  lrs = [0.001]\n",
    "  save_path = 'gridsearch/link/'\n",
    "  grid_search(hids,outs,lrs,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOAD_TEST_SET:\n",
    "  def grid_search(hidden_channels:list, out_channels:list,lrs:list,save_path:str):\n",
    "      def train(optimizer, criterion):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        #palm_oil_mask = np.where(data.y.cpu().numpy()==1)[0]\n",
    "        # data_edge_index = data.edge_index[palm_oil_mask]\n",
    "        # data_edge_label = data.edge_label[palm_oil_mask]\n",
    "        # data_edge_label_index = data.edge_label_index[palm_oil_mask]\n",
    "        \n",
    "        \n",
    "        z=model.encode(data.x, data.edge_index)\n",
    "        #print(data.edge_index.size(),data.edge_label.size(1),len(data.x))\n",
    "        neg_edge_index = negative_sampling(edge_index=data.edge_index, num_nodes=len(data.x), num_neg_samples=data.edge_label_index.size(1), method=\"sparse\")\n",
    "        edge_label_index = torch.cat([data.edge_label_index, neg_edge_index], dim=-1)\n",
    "        edge_label = torch.cat([data.edge_label, data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "\n",
    "\n",
    "        out = model.decode(z,edge_label_index).view(-1)\n",
    "        loss = criterion(out,edge_label)\n",
    "        # out = model.decode(z,data.edge_label_index).view(-1)\n",
    "        # loss = criterion(out, data.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = out.cpu().detach().numpy()\n",
    "        preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "        acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "        return loss\n",
    "      \n",
    "      @torch.no_grad()\n",
    "      def test():\n",
    "            model.eval()\n",
    "            z = model.encode(test_data.x, test_data.edge_index)\n",
    "            neg_edge_index = negative_sampling(edge_index=test_data.edge_index, num_nodes=len(test_data.x), num_neg_samples=test_data.edge_label_index.size(1), method=\"sparse\")\n",
    "            edge_label_index = torch.cat([test_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "            edge_label = torch.cat([test_data.edge_label, test_data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "            out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "            \n",
    "            preds = out.cpu().numpy()\n",
    "            preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "            el = edge_label.cpu().numpy()\n",
    "            acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "\n",
    "            f1 = f1_score(edge_label.cpu().numpy(), preds)\n",
    "\n",
    "            #print(len(np.where(o>=0.5)[0]), len(np.where(o<=0.5)[0]), len(o))\n",
    "            return roc_auc_score(edge_label.cpu().numpy(), out.cpu().numpy()), acc, f1\n",
    "      \n",
    "      @torch.no_grad()\n",
    "      def val():\n",
    "            model.eval()\n",
    "            z = model.encode(val_data.x, val_data.edge_index)\n",
    "            neg_edge_index = negative_sampling(edge_index=val_data.edge_index, num_nodes=len(val_data.x), num_neg_samples=val_data.edge_label_index.size(1), method=\"sparse\")\n",
    "            edge_label_index = torch.cat([val_data.edge_label_index, neg_edge_index], dim=-1)\n",
    "            edge_label = torch.cat([val_data.edge_label, val_data.edge_label.new_zeros(neg_edge_index.size(1))], dim=0)\n",
    "            out = model.decode(z, edge_label_index).view(-1).sigmoid()\n",
    "\n",
    "            preds = out.cpu().numpy()\n",
    "            preds = np.array([0 if x<0.5 else 1 for x in preds])\n",
    "            el = edge_label.cpu().numpy()\n",
    "            o =  out.cpu().numpy()\n",
    "\n",
    "            f1 = f1_score(edge_label.cpu().numpy(), preds)\n",
    "            \n",
    "\n",
    "            acc =  float(len(np.where(preds==edge_label.cpu().numpy())[0]))/len(preds)\n",
    "            #print(len(np.where(o>=0.5)[0]), len(np.where(o<=0.5)[0]), len(o))\n",
    "            return roc_auc_score(edge_label.cpu().numpy(), out.cpu().numpy()), acc,f1\n",
    "\n",
    "      num_features = len(data.x[0])\n",
    "      data_dict = {}\n",
    "      end_str = INGREDIENT_TO_REPLACE\n",
    "      data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{}, 'Validation AUC':{}, 'Validation F1': {}, 'Test F1': {}}\n",
    "      for hidden_chans in hidden_channels:\n",
    "        for out_chans in out_channels:\n",
    "          for lr in lrs:\n",
    "            for i in range(25):\n",
    "              model = SAGENet(num_features, hidden_chans, out_chans).to(device)\n",
    "              optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "              criterion = torch.nn.BCEWithLogitsLoss()\n",
    "              max_acc = 0.0\n",
    "              max_auc = 0.0\n",
    "              acc_li = []\n",
    "              auc_li = []\n",
    "              test_acc_li = []\n",
    "              test_auc_li = []\n",
    "              test_f1_li = []\n",
    "              val_f1_li = []\n",
    "              loss_li = []\n",
    "              stop_at = 0\n",
    "              for epoch in range(1,501):\n",
    "                loss=train(optimizer,criterion)         \n",
    "                loss_li.append(loss.cpu().detach().item())\n",
    "                if (epoch%10)==0 or epoch==1:\n",
    "                  auc,acc,val_f1 = val()\n",
    "                  acc_li.append(acc)\n",
    "                  auc_li.append(auc)\n",
    "                  val_f1_li.append(val_f1)\n",
    "                  test_auc, test_acc,test_f1 = test()\n",
    "                  test_acc_li.append(test_acc)\n",
    "                  test_auc_li.append(test_auc)\n",
    "                  test_f1_li.append(test_f1)\n",
    "                  if acc > max_acc:\n",
    "                    stop_at = epoch\n",
    "                    max_auc = auc \n",
    "                    max_acc = acc \n",
    "                    torch.save(model.state_dict(),f'{gnn_path}_{i}') \n",
    "                # print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "                #       f'Test AUC: {test_auc:.4f}',f'Max AUC: {max_auc:.4f}')\n",
    "\n",
    "              print(f'{i} -- Hids: {hidden_chans} Outs: {out_chans} LR: {lr} Max AUC: {auc_li[int(stop_at/10)]:.4f} Max Acc: {acc_li[int(stop_at/10)]: .4f} Max F1: {val_f1_li[int(stop_at/10)]}')\n",
    "\n",
    "              data_dict[end_str]['train_loss'][i] = loss_li\n",
    "              data_dict[end_str]['Validation Accuracy'][i] = acc_li\n",
    "              data_dict[end_str]['Validation AUC'][i] = auc_li\n",
    "              data_dict[end_str]['Test Accuracy'][i] = test_acc_li\n",
    "              data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "              data_dict[end_str]['Validation F1'][i] = val_f1_li\n",
    "              data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "              del(model)\n",
    "            with open(save_path+f'{INGREDIENT_TO_REPLACE}_data_dict_entropy_stop_{stop_at}', 'wb') as f:\n",
    "              pickle.dump(data_dict, f)\n",
    "        \n",
    "  outs = [128] \n",
    "  hids = [128] \n",
    "  lrs = [0.001]\n",
    "  save_path = 'gridsearch/link/'\n",
    "  grid_search(hids,outs,lrs,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_graph = to_networkx(final_data)\n",
    "# pos=nx.circular_layout(new_graph)\n",
    "# nx.draw(new_graph, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.y)\n",
    "# new_graph = to_networkx(data)\n",
    "# pos=nx.kamada_kawai_layout(new_graph)\n",
    "\n",
    "# k=20\n",
    "\n",
    "# sampled_nodes = random.sample(new_graph.nodes,k)\n",
    "\n",
    "# sampled_graph = new_graph.subgraph(sampled_nodes)\n",
    "\n",
    "# print(nx.is_bipartite(sampled_graph))\n",
    "\n",
    "# li = list(sampled_nodes)\n",
    "# cm = []\n",
    "# palm_list = []\n",
    "# o_c, b_c = 0,0\n",
    "# for node in sampled_graph:\n",
    "#     if data.y[node] ==1:\n",
    "#         cm.append('orange')\n",
    "#         palm_list.append(node)\n",
    "#         o_c+=1\n",
    "#     else:\n",
    "#         cm.append('blue')\n",
    "#         b_c+=1\n",
    "\n",
    "# pos=nx.bipartite_layout(sampled_graph, nodes=palm_list)\n",
    "# nx.draw(sampled_graph, node_color=cm, pos=pos, edgecolors='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
