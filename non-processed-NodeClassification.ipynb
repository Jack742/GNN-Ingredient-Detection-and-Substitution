{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'USAGE:\\nIf Training GNN -> set the following to true\\n|MAKE_TEST_SET\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\nIf Testing GNN -> set the following to true\\n|LOAD_TEST_SET\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\nIf Training MLP/XGBOOST -> set the following to true\\n|USE_BRANDS_AND_INGR\\n|LOAD_BALANCED_DATA\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\nIf Testing MLP/XGBOOST -> set the following to true\\n|USE_BRANDS_AND_INGR\\n|LOAD_TEST_SET\\n|STEM\\n|DECOMPOSE_HIERARCHY\\n\\n||ORDER OF RUNNING||\\n| RUN TRAIN-GNN FIRST, THEN TEST-GNN, THEN TRAIN MLP/XG, THEN TEST-MLP/XG\\n| ENSURE FRAC IS ALWAYS\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import random\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import json\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import sys\n",
    "from torch_geometric.data import Data\n",
    "from torch.nn import Dropout\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import torch_geometric.transforms as transforms\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "import pandas as pd\n",
    "#www.python-graph-gallery.com\n",
    "\n",
    "\n",
    "USE_FULL_FEATURES = False\n",
    "USE_BRANDS_AND_INGR = True\n",
    "USE_FREQUENCY = False\n",
    "MASK = False\n",
    "\n",
    "\n",
    "\n",
    "LOAD_TEST_SET = True\n",
    "LOAD_BALANCED_DATA = True\n",
    "MAKE_TEST_SET = False\n",
    "\n",
    "SAMPLE_FRAC = 0.75\n",
    "TEST_SAMPLE_FRAC = 0.25\n",
    "STEM = True\n",
    "DECOMPOSE_HIERARCHY = True\n",
    "REMOVE_REPLACE = False\n",
    "\n",
    "INGREDIENT_TO_CLASSIFY = 'soy'#\n",
    "\n",
    "\"\"\"USAGE:\n",
    "If Training GNN -> set the following to true\n",
    "|MAKE_TEST_SET\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "If Testing GNN -> set the following to true\n",
    "|LOAD_TEST_SET\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "If Training MLP/XGBOOST -> set the following to true\n",
    "|USE_BRANDS_AND_INGR\n",
    "|LOAD_BALANCED_DATA\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "If Testing MLP/XGBOOST -> set the following to true\n",
    "|USE_BRANDS_AND_INGR\n",
    "|LOAD_TEST_SET\n",
    "|STEM\n",
    "|DECOMPOSE_HIERARCHY\n",
    "\n",
    "||ORDER OF RUNNING||\n",
    "| RUN TRAIN-GNN FIRST, THEN TEST-GNN, THEN TRAIN MLP/XG, THEN TEST-MLP/XG\n",
    "| ENSURE FRAC IS ALWAYS\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data From File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data()->pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from file, depending on global config parameters set\n",
    "    \"\"\"\n",
    "    if LOAD_BALANCED_DATA:\n",
    "        df = pd.read_csv(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_balanced_data.csv')\n",
    "    elif LOAD_TEST_SET:\n",
    "        df = pd.read_csv(f'test_set.csv')\n",
    "    else:\n",
    "        files = [\n",
    "            \"outputs/food_1_Y.csv\",\n",
    "            \"outputs/food_2_Y.csv\",\n",
    "            \"outputs/food_3_Y.csv\"\n",
    "        ]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        for file in files:\n",
    "            df = pd.concat([df, pd.read_csv(file)])\n",
    "\n",
    "        #df = df[df.index < int(len(df.index)/50)]\n",
    "        if MAKE_TEST_SET:\n",
    "            df1 = df.sample(frac=TEST_SAMPLE_FRAC, random_state=123456789)\n",
    "            df = df.drop(df1.index)\n",
    "            df = df.sample(frac=SAMPLE_FRAC, random_state=123456789)\n",
    "            df1.reset_index()\n",
    "            df = df.reset_index()\n",
    "            df1.to_csv('test_set.csv')\n",
    "        else:\n",
    "            df = df.sample(frac=SAMPLE_FRAC, random_state=123456789)\n",
    "            df.reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stopwords = []\n",
    "        for l in f:\n",
    "            if l not in stopwords:\n",
    "                stopwords.append(l.replace('\\n',''))\n",
    "    return stopwords\n",
    "\n",
    "def get_hierarchy_ingreds():\n",
    "    with open('replace_words.json') as f:\n",
    "        replacements = json.load(f)\n",
    "        for each in replacements.keys():\n",
    "            replacements[each] = ast.literal_eval(replacements[each])  \n",
    "    return replacements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utility Functions\n",
    "\"\"\"\n",
    "\n",
    "def stem_ingreds(row: pd.Series, ingred_idx: int)->None:\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in INGREDIENT_TO_CLASSIFY.split(' ')]\n",
    "    s = ''\n",
    "    for word in words:\n",
    "        s += word\n",
    "    sentence = row['ingredients_list'][ingred_idx]\n",
    "    words = [ps.stem(word) for word in sentence.split(' ')]\n",
    "    s = ''\n",
    "    for word in words:\n",
    "        s += word    \n",
    "    row['ingredients_list'][ingred_idx] = s\n",
    "    row['ingredients_list'][ingred_idx] = row['ingredients_list'][ingred_idx].replace(' ', '')\n",
    "\n",
    "\n",
    "def decompose_hierarchy(row: pd.Series, replacements: dict) -> None: \n",
    "    idx = 0\n",
    "    idx_to_remove = []\n",
    "    while idx<len(row['ingredients_list']):\n",
    "        if row['ingredients_list'][idx] in replacements.keys():\n",
    "            #NEED TO DO THIS. SIMPLE GET THE VALUES IN replacements. CAN'T APPEND IN LOOP, BUT YOU COULD DO A WHILE LOOP\n",
    "            idx_to_remove.append(idx)\n",
    "            if LOAD_TEST_SET:                \n",
    "                if all(item.lower() in replacements[row['ingredients_list'][idx]] \\\n",
    "                    for item in INGREDIENT_TO_CLASSIFY.lower().split()):\n",
    "                    row['ingredients_list'].append(INGREDIENT_TO_CLASSIFY)\n",
    "            else:\n",
    "                row['ingredients_list'].extend(replacements[row['ingredients_list'][idx]])             \n",
    "        idx+=1\n",
    "    if REMOVE_REPLACE:\n",
    "        for i in reversed(idx_to_remove):\n",
    "            row['ingredients_list'].pop(i)\n",
    "\n",
    "def remove_stopwords(row:pd.Series,ingred_idx:int, stopwords: list):\n",
    "    stop_row_li = np.array(row['ingredients_list'][ingred_idx].split(' '))\n",
    "    for stopword in stopwords:\n",
    "            stop_loc = np.where(stop_row_li ==stopword)\n",
    "            if len(stop_loc[0])>0:\n",
    "                stop_row_li= np.delete(stop_row_li,stop_loc)\n",
    "                str_ = ''\n",
    "                for stringify_ in stop_row_li:\n",
    "                    str_ += ' ' + stringify_\n",
    "                row['ingredients_list'][ingred_idx] = str_\n",
    "\n",
    "def is_sparse(row: pd.Series) -> bool:\n",
    "    if row['brand'] == math.nan:\n",
    "        return True\n",
    "\n",
    "    len_ing = len(row['ingredients_list'])\n",
    "    if len_ing==0:\n",
    "        return True\n",
    "    elif len_ing == 1 and row['ingredients_list'][0].lower() in ['nan', 'none', 'na']:\n",
    "        return True\n",
    "\n",
    "def combine_all_labels(row: pd.Series, ingred_idx: int)->None:\n",
    "    each = row['ingredients_list'][ingred_idx]\n",
    "    if all(item in each.lower() for item in INGREDIENT_TO_CLASSIFY.lower().split()):       \n",
    "        row['ingredients_list'][ingred_idx] = INGREDIENT_TO_CLASSIFY.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4227it [00:01, 3730.47it/s]\n",
      "4227it [02:01, 34.75it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>brand</th>\n",
       "      <th>categories</th>\n",
       "      <th>price_max</th>\n",
       "      <th>price_min</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>ingredients_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Newman's Own Cookies</td>\n",
       "      <td>Newman's Own</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Cookies', 'Chocola...</td>\n",
       "      <td>30.72</td>\n",
       "      <td>30.72</td>\n",
       "      <td>['unbleached wheat flour', 'sugar', 'chocolate...</td>\n",
       "      <td>[wheatflour, sugar, chocolchipchocol, sugar, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M and Ms Fun Size Milk Chocolate - Brown Candy...</td>\n",
       "      <td>A Great Surprise</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Candy &amp; Chocolate'...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>24.99</td>\n",
       "      <td>['milk chocolate sugar', 'chocolate', 'skim mi...</td>\n",
       "      <td>[milkchocolsugar, chocol, milk, cocoabutter, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NOH Chinese Char Siu Mis, Barbecue, 2.5 Oz</td>\n",
       "      <td>NOH Foods</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Condiments', ' Sau...</td>\n",
       "      <td>10.93</td>\n",
       "      <td>5.49</td>\n",
       "      <td>['cane sugar', 'powdered soy sauce wheat', 'so...</td>\n",
       "      <td>[canesugar, soy, soy, salt, maltodextrin, onio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Devour Sharp Cheddar with Bacon Mac &amp; Cheese B...</td>\n",
       "      <td>Devour</td>\n",
       "      <td>['Household &amp; Grocery', 'Food &amp; Snacks', 'Cann...</td>\n",
       "      <td>3.99</td>\n",
       "      <td>3.99</td>\n",
       "      <td>['sodium phosphate', 'contains less than of sa...</td>\n",
       "      <td>[sodiumphosphat, salt, sodiumalgin, spiceceler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Nonni's Biscotti</td>\n",
       "      <td>Nonni's</td>\n",
       "      <td>['Biscotti', 'Grocery &amp; Gourmet Food', 'Cookie...</td>\n",
       "      <td>32.20</td>\n",
       "      <td>30.88</td>\n",
       "      <td>['wheat flour', 'sugar', 'bittersweet chocolat...</td>\n",
       "      <td>[wheatflour, sugar, bittersweetchocolchocolpro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>4222</td>\n",
       "      <td>Extra Large Rock Candy Sticks (22g): 24 Blue R...</td>\n",
       "      <td>Espeez</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Candy &amp; Chocolate'...</td>\n",
       "      <td>24.97</td>\n",
       "      <td>24.97</td>\n",
       "      <td>['pure cane sugar', 'less than of the followin...</td>\n",
       "      <td>[canesugar, flavor, red, blue, red, yellow, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>4223</td>\n",
       "      <td>Base Culture Almond Butter Brownie All Natural...</td>\n",
       "      <td>Base Culture</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Breads &amp; Bakery', ...</td>\n",
       "      <td>5.50</td>\n",
       "      <td>5.50</td>\n",
       "      <td>['honey', 'egg', 'cashew butter', 'coconut oil...</td>\n",
       "      <td>[honey, egg, cashewbutter, coconutoil, chocol,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4224</th>\n",
       "      <td>4224</td>\n",
       "      <td>Mott's No Sugar Added Applesauce</td>\n",
       "      <td>Mott's</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Applesauce &amp; Fruit...</td>\n",
       "      <td>5.88</td>\n",
       "      <td>5.88</td>\n",
       "      <td>['apples', 'water', 'ascorbic acid .']</td>\n",
       "      <td>[appl, water, ascorbacid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4225</th>\n",
       "      <td>4225</td>\n",
       "      <td>Special K Breakfast Cereal Honey Oat</td>\n",
       "      <td>Special K</td>\n",
       "      <td>['Breakfast Foods', 'Cold Cereals', 'Grocery &amp;...</td>\n",
       "      <td>14.99</td>\n",
       "      <td>14.99</td>\n",
       "      <td>['whole grain wheat', 'sugar', 'rice', 'whole ...</td>\n",
       "      <td>[wholegrainwheat, sugar, rice, wholegrainoat, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4226</th>\n",
       "      <td>4226</td>\n",
       "      <td>Del Monte Canned Yellow Cling Sliced Peaches i...</td>\n",
       "      <td>Del Monte</td>\n",
       "      <td>['Grocery &amp; Gourmet Food', 'Pantry Staples', '...</td>\n",
       "      <td>21.12</td>\n",
       "      <td>21.12</td>\n",
       "      <td>['can del monte canned peaches', 'not drained'...</td>\n",
       "      <td>[delmontcanpeach, drain, peach, water, fructos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4227 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               name  \\\n",
       "0         0                               Newman's Own Cookies   \n",
       "1         1  M and Ms Fun Size Milk Chocolate - Brown Candy...   \n",
       "2         2         NOH Chinese Char Siu Mis, Barbecue, 2.5 Oz   \n",
       "3         3  Devour Sharp Cheddar with Bacon Mac & Cheese B...   \n",
       "4         4                                   Nonni's Biscotti   \n",
       "...     ...                                                ...   \n",
       "4222   4222  Extra Large Rock Candy Sticks (22g): 24 Blue R...   \n",
       "4223   4223  Base Culture Almond Butter Brownie All Natural...   \n",
       "4224   4224                   Mott's No Sugar Added Applesauce   \n",
       "4225   4225               Special K Breakfast Cereal Honey Oat   \n",
       "4226   4226  Del Monte Canned Yellow Cling Sliced Peaches i...   \n",
       "\n",
       "                 brand                                         categories  \\\n",
       "0         Newman's Own  ['Grocery & Gourmet Food', 'Cookies', 'Chocola...   \n",
       "1     A Great Surprise  ['Grocery & Gourmet Food', 'Candy & Chocolate'...   \n",
       "2            NOH Foods  ['Grocery & Gourmet Food', 'Condiments', ' Sau...   \n",
       "3               Devour  ['Household & Grocery', 'Food & Snacks', 'Cann...   \n",
       "4              Nonni's  ['Biscotti', 'Grocery & Gourmet Food', 'Cookie...   \n",
       "...                ...                                                ...   \n",
       "4222            Espeez  ['Grocery & Gourmet Food', 'Candy & Chocolate'...   \n",
       "4223      Base Culture  ['Grocery & Gourmet Food', 'Breads & Bakery', ...   \n",
       "4224            Mott's  ['Grocery & Gourmet Food', 'Applesauce & Fruit...   \n",
       "4225         Special K  ['Breakfast Foods', 'Cold Cereals', 'Grocery &...   \n",
       "4226         Del Monte  ['Grocery & Gourmet Food', 'Pantry Staples', '...   \n",
       "\n",
       "      price_max  price_min                                        ingredients  \\\n",
       "0         30.72      30.72  ['unbleached wheat flour', 'sugar', 'chocolate...   \n",
       "1         24.99      24.99  ['milk chocolate sugar', 'chocolate', 'skim mi...   \n",
       "2         10.93       5.49  ['cane sugar', 'powdered soy sauce wheat', 'so...   \n",
       "3          3.99       3.99  ['sodium phosphate', 'contains less than of sa...   \n",
       "4         32.20      30.88  ['wheat flour', 'sugar', 'bittersweet chocolat...   \n",
       "...         ...        ...                                                ...   \n",
       "4222      24.97      24.97  ['pure cane sugar', 'less than of the followin...   \n",
       "4223       5.50       5.50  ['honey', 'egg', 'cashew butter', 'coconut oil...   \n",
       "4224       5.88       5.88             ['apples', 'water', 'ascorbic acid .']   \n",
       "4225      14.99      14.99  ['whole grain wheat', 'sugar', 'rice', 'whole ...   \n",
       "4226      21.12      21.12  ['can del monte canned peaches', 'not drained'...   \n",
       "\n",
       "                                       ingredients_list  \n",
       "0     [wheatflour, sugar, chocolchipchocol, sugar, d...  \n",
       "1     [milkchocolsugar, chocol, milk, cocoabutter, l...  \n",
       "2     [canesugar, soy, soy, salt, maltodextrin, onio...  \n",
       "3     [sodiumphosphat, salt, sodiumalgin, spiceceler...  \n",
       "4     [wheatflour, sugar, bittersweetchocolchocolpro...  \n",
       "...                                                 ...  \n",
       "4222  [canesugar, flavor, red, blue, red, yellow, ye...  \n",
       "4223  [honey, egg, cashewbutter, coconutoil, chocol,...  \n",
       "4224                          [appl, water, ascorbacid]  \n",
       "4225  [wholegrainwheat, sugar, rice, wholegrainoat, ...  \n",
       "4226  [delmontcanpeach, drain, peach, water, fructos...  \n",
       "\n",
       "[4227 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = get_stopwords()\n",
    "if DECOMPOSE_HIERARCHY:\n",
    "    replacements = get_hierarchy_ingreds()\n",
    "    \n",
    "df = get_data()\n",
    "#Add a list of the ingredients, and concatenate palm oil products to be just palm oil\n",
    "ingredients = [[x.lower() for x in ast.literal_eval(product['ingredients'])] for _,product in df.iterrows()]\n",
    "df['ingredients_list'] = ingredients\n",
    "\n",
    "\n",
    "drop_sparse_idx = []\n",
    "\n",
    "\n",
    "trans = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "for row_idx, row in tqdm(df.iterrows()):\n",
    "    if is_sparse(row):\n",
    "        drop_sparse_idx.append(row_idx)\n",
    "    else:\n",
    "        for idx in range(len(row['ingredients_list'])):\n",
    "            row['ingredients_list'][idx] = row['ingredients_list'][idx].translate(trans) \n",
    "        if DECOMPOSE_HIERARCHY:\n",
    "            decompose_hierarchy(row, replacements)\n",
    "\n",
    "df = df.drop(drop_sparse_idx)\n",
    "drop_sparse_idx = []\n",
    "\n",
    "for row_idx, row in tqdm(df.iterrows()):\n",
    "    pop_li = []\n",
    "    for idx, each in enumerate(row['ingredients_list']):        \n",
    "        remove_stopwords(row,idx, stopwords)\n",
    "        if STEM:\n",
    "            stem_ingreds(row, idx)\n",
    "        if row['ingredients_list'][idx] == '':\n",
    "            pop_li.append(idx)\n",
    "        else:\n",
    "            combine_all_labels(row, idx)\n",
    "    if pop_li:\n",
    "        for each in reversed(pop_li):\n",
    "            row['ingredients_list'].pop(each)\n",
    "        if len(row['ingredients_list']) == 0:\n",
    "            drop_sparse_idx.append(row_idx)\n",
    "\n",
    "df = df.drop(drop_sparse_idx)\n",
    "df = df.drop_duplicates(subset=\"name\")\n",
    "\n",
    "\n",
    "df = df[['name','brand','categories','price_max','price_min','ingredients','ingredients_list']]\n",
    "df = df.dropna().reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = df['name'].unique()\n",
    "unique_brands = df['brand'].unique()\n",
    "\n",
    "unique_cats = np.unique(sum(df[\"categories\"].apply(ast.literal_eval).values.tolist(), []))\n",
    "unique_names = sorted(unique_names)\n",
    "\n",
    "unique_ingred = []\n",
    "for x in df['ingredients_list']:\n",
    "    for y in x:\n",
    "        if y not in unique_ingred:\n",
    "            unique_ingred.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_matrix():\n",
    "    feature_mat = np.zeros((len(unique_names),len(unique_ingred)),dtype=int)\n",
    "    for idx, row in df.iterrows():\n",
    "        ingr = row['ingredients_list']\n",
    "        for ing_idx,each in enumerate(unique_ingred):\n",
    "            feature_mat[idx,ing_idx] = 1 if each in ingr else 0\n",
    "    return feature_mat\n",
    "\n",
    "def get_feature_coocurrance():    \n",
    "    feature_mat = dict()\n",
    "    for each in range(len(unique_ingred)):\n",
    "        feature_mat[each] = np.zeros((len(unique_ingred)),dtype=int)\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        ingr = row['ingredients_list']#ast.literal_eval(row['ingredients'])\n",
    "        for ing_idx,each in enumerate(unique_ingred):\n",
    "            if each in ingr:\n",
    "                for ing_idx2,each2 in enumerate(unique_ingred):\n",
    "                    if each2 in ingr:\n",
    "                        #Minus the value along the diagonal. Prevents inteference with the coocurrance but also allows us to retrieve magnitude later\n",
    "                        feature_mat[ing_idx][ing_idx2] += 1 if ing_idx!=ing_idx2 else -1\n",
    "    return feature_mat\n",
    "\n",
    "def get_cooccurance_tf_idf(palm_oil_cooccurance_vector):\n",
    "    # Adapted from TF-IDF.\n",
    "    # IDF = log(n/j) where n = total number of documents and j = number of documents containing a given term. This is kept the same, but squared\n",
    "    # TF is the frquency of a term in a *given* document. This doesn't really apply here, but instead we..\n",
    "    # || consider the frequency of a term (i.e. ingredient) in products that also contain palm oil (i.e. co-occurance)\n",
    "\n",
    "    n = len(unique_names)\n",
    "\n",
    "    #IMPORTANT We initialise the array to 1 rather than 0 at present, this is to avoid 0 divides in the math.log without having to iterate over idf to check and then skip the log step due to time complexity\n",
    "    idf = np.ones(len(unique_ingred))\n",
    "    for idx,row in df.iterrows():\n",
    "        ingr_list = row['ingredients_list']#ast.literal_eval(row['ingredients'])\n",
    "        for i,ingredient in enumerate(unique_ingred):\n",
    "            idf[i] += 1 if ingredient in ingr_list else 0\n",
    "        \n",
    "        \n",
    "        idf = np.array([math.log(n/j) for j in idf])\n",
    "    #assert len(palm_oil_cooccurance_vector) == len(idf)\n",
    "\n",
    "    tf_idf_vector = palm_oil_cooccurance_vector * np.square(idf)\n",
    "    return tf_idf_vector\n",
    "\n",
    "def get_information_entropy(cooccurance_matrix, palm_oil_idx, num_palm_oil, num_total_products):\n",
    "    #Intuition: use information entropy to calculate how much information each other ingredient can give us on the presence of palm oil\n",
    "    # - (P(palm_oil_presence|x) log(P(palm_oil_presence|x))) + (P(¬palm_oil_presence|x) log(P(¬palm_oil_presence|x)))\n",
    "    def calculate_entropy(palm_presence, n):\n",
    "        if n != 0:\n",
    "            p1 = palm_presence/n\n",
    "        else:\n",
    "            p1 = 0\n",
    "        p2 = 1-p1\n",
    "        sum1 = 0 if p1==0 else (p1*math.log2(p1))\n",
    "        sum2 = 0 if p2==0 else (p2*math.log2(p2))\n",
    "        return - sum1 + sum2\n",
    "    \n",
    "    def calcalate_conditional_entropy(palm_presence, n, p_n):\n",
    "        if p_n < 0:\n",
    "            print(p_n)\n",
    "            exit()\n",
    "        if n!= 0:\n",
    "            p1 = (palm_presence/n)*p_n\n",
    "        else:\n",
    "            p1 = 0\n",
    "        assert p1 >=0\n",
    "        assert p_n >=0\n",
    "        sum1 = 1 if (p1<=0 or p_n<=0) else (1 + (p1*math.log2(1+(p1/p_n))))\n",
    "        if sum1 <= 0:\n",
    "            print(p1, p_n, palm_presence, n)\n",
    "        return sum1\n",
    "\n",
    "    n = len(unique_ingred)\n",
    "    entropies = np.empty(n) \n",
    "    for idx,each in enumerate(cooccurance_matrix[palm_oil_idx]):\n",
    "        if idx == palm_oil_idx:\n",
    "            entropies[idx] == 100\n",
    "        else:\n",
    "            n_occ = -cooccurance_matrix[idx][idx]\n",
    "            assert n_occ >= each, (unique_ingred[idx])\n",
    "            entropies[idx] = (calcalate_conditional_entropy(each, n_occ, n_occ/num_total_products) + \n",
    "                                calcalate_conditional_entropy(each, num_total_products-n_occ, (num_total_products-n_occ)/num_total_products) + \n",
    "                                calcalate_conditional_entropy(n_occ-each, n_occ, n_occ/num_total_products) + \n",
    "                                calcalate_conditional_entropy(n_occ-each, num_total_products-n_occ, (num_total_products-n_occ)/num_total_products)\n",
    "                                )\n",
    "    return entropies        \n",
    "    # for idx, row in df.iterrows():\n",
    "    #     ingr_list = ast.literal_eval(row['ingredients'])\n",
    "    #     for i, ingredient in enumerate(ingr_list):\n",
    "\n",
    "\n",
    "        \n",
    "def show_frequency():\n",
    "  cooccurance_matrix = get_feature_coocurrance()\n",
    "  freqs = []\n",
    "  total_products = len(df.index)\n",
    "  for idx in range(len(cooccurance_matrix[0])):\n",
    "    freqs.append((-cooccurance_matrix[idx][idx])/total_products)\n",
    "  plt.bar(range(len(freqs)), sorted(freqs))\n",
    "  plt.show()\n",
    "\n",
    "def piecewise_frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies, threshold_perc = 0.01):\n",
    "  frequency_weighted_conditional_entropies = []\n",
    "  total_products = len(df.index)\n",
    "  fs = []\n",
    "  min_frequency = 0.00\n",
    "\n",
    "  def calculate_threshold():\n",
    "      freqs = []\n",
    "      total_products = len(df.index)\n",
    "      for idx in range(len(cooccurance_matrix[0])):\n",
    "        freqs.append((-cooccurance_matrix[idx][idx])/total_products)\n",
    "        \n",
    "      perc_idx = int(len(freqs)*threshold_perc)\n",
    "      top_n_threshold = sorted(freqs, reverse=True)[perc_idx]\n",
    "      zero_thresh = sorted(freqs, reverse=True)[0]\n",
    "      print(perc_idx, top_n_threshold, zero_thresh)\n",
    "      return top_n_threshold\n",
    "\n",
    "  thresh = calculate_threshold()\n",
    "  for idx in ordered_entropy:\n",
    "      #Minus as you've made the diagonal minus for other reasons      \n",
    "      if cooccurance_matrix[idx][idx]!=0:\n",
    "          #n/f = 1/(f/n), f/n == frequency.\n",
    "          fx = -cooccurance_matrix[idx][idx]\n",
    "          frequency = (fx)/total_products\n",
    "          fs.append(frequency)\n",
    "          if frequency >= min_frequency:\n",
    "              occurance = np.log(1/frequency) if frequency <= thresh else np.log(total_products/(total_products-fx))\n",
    "              if occurance < 0:\n",
    "                  print(f'less than zero, occ: {np.log(1/(total_products-fx))} with fx as {fx}')\n",
    "              \n",
    "              frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "          else:\n",
    "              frequency_weighted_conditional_entropies.append(1.0)\n",
    "  return frequency_weighted_conditional_entropies\n",
    "\n",
    "\n",
    "def frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies):\n",
    "  frequency_weighted_conditional_entropies = []\n",
    "  total_products = len(df.index)\n",
    "  fs = []\n",
    "  min_frequency = 0.00\n",
    "  for idx in ordered_entropy:\n",
    "      #Minus as you've made the diagonal minus for other reasons      \n",
    "      if cooccurance_matrix[idx][idx]!=0:\n",
    "          frequency = (-cooccurance_matrix[idx][idx])/total_products\n",
    "          fs.append(frequency)\n",
    "          if frequency >= min_frequency:\n",
    "              occurance = np.log(1/frequency)\n",
    "              if occurance < 0:\n",
    "                  print(\"yes\")\n",
    "              \n",
    "              frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "          else:\n",
    "              frequency_weighted_conditional_entropies.append(1.0)\n",
    "  return frequency_weighted_conditional_entropies\n",
    "\n",
    "\n",
    "def apply_entropy_mask(top_n=2000, frequency_weighted=True):\n",
    "  cooccurance_matrix = get_feature_coocurrance()\n",
    "  #Print the highest co-occuring ingredients\n",
    "  for idx,each in enumerate(unique_ingred):\n",
    "      if (INGREDIENT_TO_CLASSIFY in each.lower()):\n",
    "          palm_idx = idx\n",
    "          palm_vector = cooccurance_matrix[idx]\n",
    "          sorted_indicies = sorted(range(len(palm_vector)), key=lambda x: palm_vector[x], reverse=True)\n",
    "          print(\"MOST CO-OCCURANCES: \",[(unique_ingred[x],palm_vector[x]) for x in sorted_indicies[:10]])\n",
    "          break\n",
    "  entropies = get_information_entropy(cooccurance_matrix, palm_idx, num_palm,len(df.index))\n",
    "  ordered_entropy = sorted(range(len(entropies)), key=lambda x: entropies[x], reverse=False)\n",
    "  \n",
    "  if frequency_weighted:\n",
    "    fwce = piecewise_frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies)\n",
    "    fwce_ordered_entropy = sorted(range(len(entropies)), key=lambda x: fwce[x], reverse=False)\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in fwce_ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, fwce_ordered_entropy, unique_ingred_\n",
    "  else:\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, ordered_entropy, unique_ingred_\n",
    "\n",
    "\n",
    "def show_frequency():\n",
    "  cooccurance_matrix = get_feature_coocurrance()\n",
    "  freqs = []\n",
    "  total_products = len(df.index)\n",
    "  for idx in range(len(cooccurance_matrix[0])):\n",
    "    freqs.append((-cooccurance_matrix[idx][idx])/total_products)\n",
    "  plt.bar(range(len(freqs)), sorted(freqs))\n",
    "  plt.show()\n",
    "\n",
    "def piecewise_frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies, threshold_perc = 0.01):\n",
    "  frequency_weighted_conditional_entropies = []\n",
    "  total_products = len(df.index)\n",
    "  fs = []\n",
    "  min_frequency = 0.00\n",
    "\n",
    "  def calculate_threshold():\n",
    "      freqs = []\n",
    "      total_products = len(df.index)\n",
    "      for idx in range(len(cooccurance_matrix[0])):\n",
    "        freqs.append((-cooccurance_matrix[idx][idx])/total_products)\n",
    "        \n",
    "      perc_idx = int(len(freqs)*threshold_perc)\n",
    "      top_n_threshold = sorted(freqs, reverse=True)[perc_idx]\n",
    "      zero_thresh = sorted(freqs, reverse=True)[0]\n",
    "      print(perc_idx, top_n_threshold, zero_thresh)\n",
    "      return top_n_threshold\n",
    "\n",
    "  thresh = calculate_threshold()\n",
    "  for idx in ordered_entropy:\n",
    "      #Minus as you've made the diagonal minus for other reasons      \n",
    "      if cooccurance_matrix[idx][idx]!=0:\n",
    "          #n/f = 1/(f/n), f/n == frequency.\n",
    "          fx = -cooccurance_matrix[idx][idx]\n",
    "          frequency = (fx)/total_products\n",
    "          fs.append(frequency)\n",
    "          if frequency >= min_frequency:\n",
    "              occurance = np.log(1/frequency) if frequency <= thresh else np.log(total_products/(total_products-fx))\n",
    "              if occurance < 0:\n",
    "                  print(f'less than zero, occ: {np.log(1/(total_products-fx))} with fx as {fx}')\n",
    "              \n",
    "              frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "          else:\n",
    "              frequency_weighted_conditional_entropies.append(1.0)\n",
    "  return frequency_weighted_conditional_entropies\n",
    "\n",
    "\n",
    "def frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies):\n",
    "  frequency_weighted_conditional_entropies = []\n",
    "  total_products = len(df.index)\n",
    "  fs = []\n",
    "  min_frequency = 0.00\n",
    "  for idx in ordered_entropy:\n",
    "      #Minus as you've made the diagonal minus for other reasons      \n",
    "      if cooccurance_matrix[idx][idx]!=0:\n",
    "          frequency = (-cooccurance_matrix[idx][idx])/total_products\n",
    "          fs.append(frequency)\n",
    "          if frequency >= min_frequency:\n",
    "              occurance = np.log(1/frequency)\n",
    "              if occurance < 0:\n",
    "                  print(\"yes\")\n",
    "              \n",
    "              frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "          else:\n",
    "              frequency_weighted_conditional_entropies.append(1.0)\n",
    "  return frequency_weighted_conditional_entropies\n",
    "\n",
    "\n",
    "def apply_entropy_mask(feature_matrix,top_n=2000, frequency_weighted=True):\n",
    "  cooccurance_matrix = get_feature_coocurrance()\n",
    "  #Print the highest co-occuring ingredients\n",
    "  for idx,each in enumerate(unique_ingred):\n",
    "      if (INGREDIENT_TO_CLASSIFY in each.lower()):\n",
    "          palm_idx = idx\n",
    "          palm_vector = cooccurance_matrix[idx]\n",
    "          sorted_indicies = sorted(range(len(palm_vector)), key=lambda x: palm_vector[x], reverse=True)\n",
    "          print(\"MOST CO-OCCURANCES: \",[(unique_ingred[x],palm_vector[x]) for x in sorted_indicies[:10]])\n",
    "          break\n",
    "  entropies = get_information_entropy(cooccurance_matrix, palm_idx, num_palm,len(df.index))\n",
    "  ordered_entropy = sorted(range(len(entropies)), key=lambda x: entropies[x], reverse=False)\n",
    "  \n",
    "  if frequency_weighted:\n",
    "    fwce = piecewise_frequency_weighted_conditional_entropies(cooccurance_matrix, ordered_entropy, entropies)\n",
    "    fwce_ordered_entropy = sorted(range(len(entropies)), key=lambda x: fwce[x], reverse=False)\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in fwce_ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, fwce_ordered_entropy, unique_ingred_\n",
    "  else:\n",
    "    unique_ingred_ = [unique_ingred[i_] for i_ in ordered_entropy[:top_n]]\n",
    "    feature_matrix = get_feature_matrix() \n",
    "    return feature_matrix, ordered_entropy, unique_ingred_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adj_matrix_from_features(feature_mat):\n",
    "    adj_mat = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        for i in elements:\n",
    "            for j in elements:\n",
    "                adj_mat[i,j] += 1 if i!=j else 0 \n",
    "    return adj_mat\n",
    "\n",
    "def get_adj_matrix_from_extra_features(feature_mat):\n",
    "    adj_mat = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    shared_features = np.zeros((len(unique_names), len(unique_names)), dtype=int)\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        for i in elements:\n",
    "            for j in elements:\n",
    "                adj_mat[i,j] += 1 if i!=j else 0\n",
    "                shared_features[i,j] = idx\n",
    "    return (adj_mat, shared_features)\n",
    "\n",
    "def get_adj_list_from_features(graph,feature_mat):\n",
    "    adj_list = dict()\n",
    "\n",
    "    # for node in range(len(list(graph.vs))):\n",
    "    #     node_idx = node\n",
    "    #     adj_list[node_idx] = {}\n",
    "    for idx, col in enumerate(feature_mat.T):\n",
    "        elements = np.where(col==1)[0]\n",
    "        if len(elements) > 0:\n",
    "            for i in elements:\n",
    "                assert len([_ for _ in elements]) > 0\n",
    "                adj_list[i] = [_ for _ in elements]\n",
    "                adj_list[i].remove(i)\n",
    "    for each in adj_list.keys():\n",
    "        adj_list[each] = list(set(adj_list[each]))\n",
    "    return adj_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6994064])\n",
      "Data(x=[4227, 10605], edge_index=[2, 6994064], edge_attr=[6994064], y=[4227], edge_weight=[6994064], train_mask=[4227], val_mask=[4227], test_mask=[4227])\n"
     ]
    }
   ],
   "source": [
    "def get_feature_vector_and_label(product, decision_ingredients):\n",
    "  #Build feature vector of high entropy ingredients. \n",
    "  feature_vector = np.zeros(len(decision_ingredients))\n",
    "  ingredients = product['ingredients_list']#ast.literal_eval(product['ingredients'])\n",
    "  for dec_idx,decision_ingr in enumerate(decision_ingredients):\n",
    "    #Check if ingredients is in feature list, and also make sure to mask the palm oil ingredient\n",
    "    if unique_ingred[dec_idx] in ingredients and unique_ingred[dec_idx] != INGREDIENT_TO_CLASSIFY:      \n",
    "      feature_vector[dec_idx] = 1\n",
    "  #1 = palmy, 0 = non_palmy\n",
    "  label = 1 if INGREDIENT_TO_CLASSIFY in ingredients else 0\n",
    "\n",
    "  return (feature_vector, label)\n",
    "\n",
    "def get_one_feature_vector(product, feature_key,unique_features, is_list=False):\n",
    "  product_data = ast.literal_eval(product[feature_key]) if is_list else product[feature_key]\n",
    "  feature_vector = np.zeros(len(unique_features))\n",
    "  for feat_idx,feat in enumerate(unique_features):\n",
    "    if (is_list and (feat in product_data)) or (feat==product_data):\n",
    "      feature_vector[feat_idx] += 1\n",
    "  return feature_vector\n",
    "\n",
    "def get_balanced_dataset(df, test_perc, val_perc):\n",
    "  if 'level_0' in df.columns:\n",
    "    df = df.drop('level_0', axis=1).reset_index()\n",
    "  palmy_prods = []\n",
    "  for idx,row in df.iterrows():\n",
    "    for each in row['ingredients_list']:    \n",
    "      if (INGREDIENT_TO_CLASSIFY in each):\n",
    "        palmy_prods.append(row)\n",
    "\n",
    "  palmy_df = pd.DataFrame(palmy_prods)\n",
    "  #What % of the dataset is a positive class?\n",
    "  proportion = len(palmy_df.index)/len(df.index)\n",
    "\n",
    "  #Sample the same amount of non-palmy products\n",
    "  df = df.drop(list(palmy_df.index))\n",
    "  palmy_df = palmy_df.drop_duplicates(subset='index')\n",
    "\n",
    "\n",
    "  palmy_test_sample_num = int(len(palmy_df)*test_perc)\n",
    "  nonpalmy_test_sample_num = int(len(df.index)*test_perc)\n",
    "  palmy_val_sample_num = int(len(palmy_df)*val_perc)\n",
    "  nonpalmy_val_sample_num = int(len(df.index)*val_perc)\n",
    "\n",
    "  sample_len = int(len(palmy_df.index)*(1-(test_perc+val_perc)))\n",
    "  non_palmy_df = df.sample(sample_len+nonpalmy_test_sample_num+nonpalmy_val_sample_num)\n",
    "  #assert len(palmy_df.index) == len(non_palmy_df.index)\n",
    "  #Shuffle each df\n",
    "  palmy_df = palmy_df.sample(frac=1)\n",
    "  non_palmy_df = non_palmy_df.sample(frac=1)\n",
    "  if 'level_0' in palmy_df.columns:\n",
    "    palmy_df = palmy_df.drop('level_0', axis=1).reset_index()\n",
    "  else:\n",
    "    palmy_df = palmy_df.reset_index()\n",
    "  if 'level_0' in non_palmy_df.columns:\n",
    "    non_palmy_df = non_palmy_df.drop('level_0', axis=1).reset_index()\n",
    "  else:\n",
    "      non_palmy_df = non_palmy_df.reset_index()\n",
    "  df=pd.concat([palmy_df, non_palmy_df]).drop('level_0', axis=1).reset_index()\n",
    "  #print(len(palmy_df.index), len(df.index),proportion, nonpalmy_test_sample_num, palmy_test_sample_num, nonpalmy_val_sample_num,palmy_val_sample_num)\n",
    "\n",
    "\n",
    "  train_mask,val_mask,test_mask = np.zeros(len(df.index)),np.zeros(len(df.index)),np.zeros(len(df.index))\n",
    "  dflen, pdflen, npdflen = len(df.index),len(palmy_df.index),len(non_palmy_df.index)\n",
    "\n",
    "  train_boundaries = (int((1-(test_perc+val_perc))*pdflen), (pdflen+npdflen-(nonpalmy_test_sample_num+nonpalmy_val_sample_num)))\n",
    "  val_boundaries = (train_boundaries[0]+int(pdflen*val_perc), train_boundaries[1]+nonpalmy_val_sample_num)\n",
    "\n",
    "  train_mask[:train_boundaries[0]] = 1\n",
    "  train_mask[pdflen:train_boundaries[1]] = 1\n",
    "\n",
    "  val_mask[train_boundaries[0]:val_boundaries[0]] = 1\n",
    "  val_mask[train_boundaries[1]:val_boundaries[1]] = 1\n",
    "\n",
    "  test_mask[val_boundaries[0]:pdflen]=1\n",
    "  test_mask[val_boundaries[1]:]=1\n",
    "\n",
    "  assert dflen == (pdflen+npdflen)\n",
    "  assert dflen == sum([sum(x) for x in [test_mask, train_mask, val_mask]])\n",
    "\n",
    "  print(sum(train_mask), sum(val_mask),sum(test_mask), len(df.index), pdflen, npdflen,palmy_test_sample_num,nonpalmy_test_sample_num)\n",
    "  return (df, torch.tensor(train_mask, dtype=bool), torch.tensor(val_mask, dtype=bool), torch.tensor(test_mask, dtype=bool))\n",
    "\n",
    "\n",
    "if LOAD_BALANCED_DATA:\n",
    "  df = pd.read_csv(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_balanced_data.csv', converters={'ingredients_list': pd.eval})\n",
    "  with open(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_test_mask.pickle', 'rb') as f:\n",
    "    test_mask = pickle.load(f)\n",
    "  with open(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_train_mask.pickle', 'rb') as f:\n",
    "    train_mask = pickle.load(f)\n",
    "  with open(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_val_mask.pickle', 'rb') as f:\n",
    "    val_mask = pickle.load(f)\n",
    "else:  \n",
    "  if LOAD_TEST_SET:\n",
    "    df,train_mask,val_mask,test_mask = get_balanced_dataset(df,0.98,0.01)\n",
    "  else:\n",
    "    df,train_mask,val_mask,test_mask = get_balanced_dataset(df,0.1,0.1)\n",
    "    df.to_csv(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_balanced_data.csv')\n",
    "    with open(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_test_mask.pickle', 'wb') as f:\n",
    "      pickle.dump(test_mask,f)\n",
    "    with open(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_train_mask.pickle', 'wb') as f:\n",
    "      pickle.dump(train_mask,f)\n",
    "    with open(f'UNBALANCED_{INGREDIENT_TO_CLASSIFY}_val_mask.pickle', 'wb') as f:\n",
    "      pickle.dump(val_mask,f)\n",
    "\n",
    "#df.to_csv('final_saved_balanced_dataset.csv')\n",
    "\n",
    "# feature_matrix = []\n",
    "# for idx, product in df.iterrows():\n",
    "#   feature_vector, label = get_feature_vector_and_label(product, unique_ingred)#get_feature_vector_and_label_with_brands(product, unique_ingred)\n",
    "#   feature_matrix.append(feature_vector)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "show_frequency()\n",
    "unique_ingred = []\n",
    "\n",
    "if LOAD_TEST_SET:\n",
    "  with open(f'{INGREDIENT_TO_CLASSIFY}test_set_unique_ingreds', 'rb') as f:\n",
    "    unique_ingred = pickle.load(f)\n",
    "  if USE_BRANDS_AND_INGR:\n",
    "    with open(f'{INGREDIENT_TO_CLASSIFY}test_set_unique_brands', 'rb') as f:\n",
    "      unique_brands = pickle.load(f)\n",
    "else:\n",
    "  if USE_BRANDS_AND_INGR:\n",
    "    with open(f'{INGREDIENT_TO_CLASSIFY}test_set_unique_brands', 'wb') as f:\n",
    "      pickle.dump(unique_brands,f)\n",
    "  if MASK:  \n",
    "    feature_matrix,ordered_entropy,unique_ingred=apply_entropy_mask(int(len(unique_ingred)/2), frequency_weighted=USE_FREQUENCY)\n",
    "  else:\n",
    "    for x in df['ingredients_list']:\n",
    "        for y in x:\n",
    "            if y not in unique_ingred:\n",
    "                unique_ingred.append(y)                \n",
    "  with open(f'{INGREDIENT_TO_CLASSIFY}test_set_unique_ingreds', 'wb') as f:\n",
    "    pickle.dump(unique_ingred, f)\n",
    "    \n",
    "#Get Matrix of Features -- Products x Features\n",
    "feature_matrix = []\n",
    "label_vector = []\n",
    "edge_matrix = []\n",
    "brand_vector = []\n",
    "cat_matrix = []\n",
    "for idx, product in df.iterrows():\n",
    "  feature_vector, label = get_feature_vector_and_label(product, unique_ingred)#get_feature_vector_and_label_with_brands(product, unique_ingred)\n",
    "  feature_matrix.append(feature_vector)\n",
    "  label_vector.append(label)\n",
    "  edge_matrix.append(get_one_feature_vector(product, 'brand',unique_brands))\n",
    "  brand_vector.append(np.where(edge_matrix[idx] == 1)[0])\n",
    "  cat_matrix.append(get_one_feature_vector(product,'categories',unique_cats, is_list=True))\n",
    "\n",
    "#(adj_matrix,feature_matrix) = drop_sparse(get_adj_matrix_from_features(np.array(feature_matrix)), feature_matrix)\n",
    "\n",
    "\n",
    "\n",
    "ingred_adj_matrix = get_adj_matrix_from_features(np.array(feature_matrix))\n",
    "#adj_matrix, shared_features = get_adj_matrix_from_extra_features(np.array(feature_matrix))\n",
    "\n",
    "if USE_FULL_FEATURES:\n",
    "  #Concatenate ingredients with brands and categories\n",
    "  feature_matrix = np.concatenate((feature_matrix, edge_matrix, cat_matrix), axis=1)\n",
    "  adj_matrix, shared_features = get_adj_matrix_from_extra_features(np.array(feature_matrix))\n",
    "elif USE_BRANDS_AND_INGR:\n",
    "  feature_matrix = np.concatenate((feature_matrix, edge_matrix), axis=1)\n",
    "  adj_matrix, shared_features = get_adj_matrix_from_extra_features(np.array(feature_matrix))\n",
    "else:\n",
    "  #Get adjacency from just brands\n",
    "  adj_matrix, shared_features = get_adj_matrix_from_extra_features(np.array(edge_matrix))\n",
    "  #adj_matrix, shared_features = get_adj_matrix_from_extra_features(np.array(cat_matrix))\n",
    "  #adj_matrix, shared_features = get_adj_matrix_from_extra_features(np.array(feature_matrix))\n",
    "\n",
    "\n",
    "# adj_matrix = get_adj_matrix_from_extra_features(np.array(feature_matrix), len(unique_ingred))\n",
    "# for col in adj_matrix:\n",
    "#   print(len(np.where(col==1)[0]))\n",
    "\n",
    "\n",
    "# if MASK:\n",
    "#   feature_matrix, ordered_entropies = apply_entropy_mask(feature_matrix, top_n=int(len(feature_matrix[:][0])/7.5))\n",
    "\n",
    "edge_index = []\n",
    "edge_attr = []\n",
    "edge_weights_ = []\n",
    "for idx,_ in enumerate(adj_matrix):\n",
    "  for target,val in enumerate(adj_matrix[idx]):\n",
    "    if val >0:\n",
    "      edge_index.append([idx,target])\n",
    "      edge_weights_.append(ingred_adj_matrix[idx, target])\n",
    "      edge_attr.append(shared_features[idx, target])\n",
    "\n",
    "#To Tensor\n",
    "feature_matrix = torch.tensor(feature_matrix, dtype=torch.float)\n",
    "label_vector = torch.tensor(label_vector)\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "print(edge_index.shape)\n",
    "edge_weights = torch.tensor([1 for _ in range(len(edge_weights_))], dtype=torch.float)#.t().contiguous()\n",
    "edge_attr = torch.tensor(edge_attr, dtype=torch.float)#.t().contiguous()\n",
    "edge_attr = torch.randn(edge_index.size(1))\n",
    "transform = transforms.Compose([\n",
    "    #transforms.RandomNodeSplit('train_rest', num_val=int(feature_matrix.shape[0]/10), num_test=int(feature_matrix.shape[0]/10)),\n",
    "    transforms.TargetIndegree()\n",
    "])\n",
    "# print(feature_matrix.shape, label_vector.shape)\n",
    "# print(len(np.where(label_vector==1)[0]),len(np.where(label_vector==0)[0]))\n",
    "\n",
    "# class ProductDataset(InMemoryDataset):\n",
    "#   def __init__(self, data, transform=None, pre_transform=None):\n",
    "#     super().__init__(None, transform, pre_transform)\n",
    "#     self.data = data\n",
    "#     self.transform = transform\n",
    "#     self.pre_transform = pre_transform \n",
    "  \n",
    "#   def len(self):\n",
    "#     return 1\n",
    "  \n",
    "#   def get(self,idx):\n",
    "#     if self.pre_transform:\n",
    "#       data = self.pre_transform(self.data)\n",
    "#     if self.transform:\n",
    "#       data = self.transform(self.data)\n",
    "#     return data\n",
    "\n",
    "# print(Data(x=feature_matrix, y=label_vector,edge_index=edge_index, edge_weight=edge_weights, edge_attr=edge_attr))\n",
    "data = transform(Data(x=feature_matrix, edge_index=edge_index))\n",
    "data.y=label_vector\n",
    "data.edge_weight=edge_weights\n",
    "data.edge_attr=edge_attr\n",
    "data.train_mask=train_mask\n",
    "data.val_mask=val_mask\n",
    "data.test_mask=test_mask\n",
    "#dataset = ProductDataset(data, transform=transform)\n",
    "#dataloader = DataLoader(dataset, \n",
    "print(data)\n",
    "\n",
    "# if MASK:\n",
    "#   masked_data = deepcopy(data)\n",
    "#   masked_data.x, ordered_entropies = apply_entropy_mask(masked_data.x, top_n=int(len(masked_data.x[:][0])/10))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "print(data.y[data.test_mask])\n",
    "new_graph = to_networkx(data)\n",
    "k=100\n",
    "\n",
    "sampled_nodes = random.sample(new_graph.nodes,k)\n",
    "\n",
    "sampled_graph = new_graph.subgraph(sampled_nodes)\n",
    "\n",
    "\n",
    "#pos=nx.kamada_kawai_layout(sampled_graph)\n",
    "cm = []\n",
    "for node in sampled_graph:\n",
    "    if data.y[node] ==1:\n",
    "        cm.append('orange')\n",
    "    else:\n",
    "        cm.append('blue')\n",
    "\n",
    "nx.draw(sampled_graph, node_color=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK and USE_FREQUENCY:\n",
    "    cooccurance_matrix = get_feature_coocurrance()\n",
    "    #Print the highest co-occuring ingredients\n",
    "    for idx,each in enumerate(unique_ingred):\n",
    "        if (INGREDIENT_TO_CLASSIFY in each.lower()):\n",
    "            palm_idx = idx\n",
    "            palm_vector = cooccurance_matrix[idx]\n",
    "            sorted_indicies = sorted(range(len(palm_vector)), key=lambda x: palm_vector[x], reverse=True)\n",
    "            print(\"MOST CO-OCCURANCES: \",[(unique_ingred[x],palm_vector[x]) for x in sorted_indicies[:10]])\n",
    "            break\n",
    "    entropies = get_information_entropy(cooccurance_matrix, palm_idx, num_palm,len(df.index))\n",
    "    ordered_entropy = sorted(range(len(entropies)), key=lambda x: entropies[x], reverse=False)\n",
    "\n",
    "    print(f\"Lowest Entropy Ingredients: {np.array(unique_ingred)[ordered_entropy[:11]]}\")\n",
    "    print(f\"Highest Entropy Ingredients: {np.array(unique_ingred)[ordered_entropy[-10:]]}\")\n",
    "\n",
    "    print(sorted(entropies)[:10])\n",
    "\n",
    "# for idx,each in enumerate(entropies):\n",
    "#     if each <=0:\n",
    "#         print(unique_ingred[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.color_palette('Set2')\n",
    "# sns.set_theme()\n",
    "if MASK and USE_FREQUENCY:\n",
    "    print(min(entropies), entropies[ordered_entropy[0]],entropies[ordered_entropy[-1]])\n",
    "\n",
    "    '''\n",
    "    Frequency-weighted conditional entropy = log(N/f) * H(Y|X)\n",
    "    Where N is total number of products and f is the number of products that contain this ingredient\n",
    "\n",
    "    '''\n",
    "    frequency_weighted_conditional_entropies = []\n",
    "    total_products = len(df.index)\n",
    "    fs = []\n",
    "    min_frequency = 0.00 #1%\n",
    "    for idx in ordered_entropy:\n",
    "        #Minus as you've made the diagonal minus for other reasons\n",
    "        \n",
    "        if cooccurance_matrix[idx][idx]!=0:\n",
    "            frequency = (-cooccurance_matrix[idx][idx])/total_products\n",
    "            fs.append(frequency)\n",
    "            if frequency >= min_frequency:\n",
    "                occurance = np.log(1/frequency)\n",
    "                if occurance < 0:\n",
    "                    print(\"yes\")\n",
    "                \n",
    "                frequency_weighted_conditional_entropies.append(occurance*entropies[idx])\n",
    "            else:\n",
    "                frequency_weighted_conditional_entropies.append(1.0)\n",
    "        else:\n",
    "            print('Entropy', unique_ingred[idx])\n",
    "    print(sorted(frequency_weighted_conditional_entropies)[:20])\n",
    "    print(min(frequency_weighted_conditional_entropies),max(frequency_weighted_conditional_entropies))\n",
    "    print(min(fs), max(fs), np.median(fs), np.mean(fs))\n",
    "\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_title('IDF-Conditional Entropy of Ingredients For Palm Oil Prediction')\n",
    "    ax.set_ylabel('IDF Conditional Entropy')\n",
    "    ax.set_xlabel('Ingredients')\n",
    "    ax.bar(np.arange(len(frequency_weighted_conditional_entropies)), sorted(frequency_weighted_conditional_entropies))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MASK and USE_FREQUENCY:\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "    ax[0].bar(np.arange(len(entropies)),sorted(entropies))\n",
    "    #ax[0].set_xticklabels(np.array(unique_ingred)[ordered_entropy], rotation=45, ha='right')\n",
    "    ax[0].set_xticks(np.arange(len(entropies)))\n",
    "\n",
    "    ax[1].bar(np.arange(len(entropies[:20])),sorted(entropies[:20]))\n",
    "    #ax[1].set_xticklabels(np.array(unique_ingred)[ordered_entropy[:20]], rotation=45, ha='right')\n",
    "    ax[1].set_xticks(np.arange(len(entropies[:20])))\n",
    "    ax[0].set_title('Conditional Entropy of Ingredients With Respect to Palm Oil')\n",
    "    ax[1].set_title('Ingredients With Lowest Conditional Entropy With Respect to Palm Oil')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv, GATv2Conv, NNConv,SAGEConv\n",
    "from torch_geometric.nn.models import AttentiveFP \n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from sklearn.manifold import TSNE\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (lin1): Linear(in_features=10605, out_features=16, bias=True)\n",
      "  (lin2): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n",
      "169730\n"
     ]
    }
   ],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(MLP, self).__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        self.lin1 = Linear(len(data.x[0]), hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels,2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(hidden_channels=16)\n",
    "print(model)\n",
    "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "\n",
    "# criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "# def train():\n",
    "#       model.train()\n",
    "#       optimizer.zero_grad()  # Clear gradients.\n",
    "#       out = model(data.x)  # Perform a single forward pass.\n",
    "#       loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "#       loss.backward()  # Derive gradients.\n",
    "#       optimizer.step()  # Update parameters based on gradients.\n",
    "#       return loss\n",
    "\n",
    "# def test():\n",
    "#       class_correct = [0,0]\n",
    "#       class_samples = [0,0]\n",
    "#       model.eval()\n",
    "#       out = model(data.x)\n",
    "#       pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#       test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "#       assert len(test_correct)==len(pred[data.test_mask])\n",
    "#       #print(test_correct)\n",
    "#       test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "#       p = pred[data.test_mask]\n",
    "#       d = data.y[data.test_mask]      \n",
    "#       for idx,each in enumerate(test_correct):\n",
    "#           class_samples[d[idx]] += 1\n",
    "#           if each == True:\n",
    "#               class_correct[p[idx]] += 1\n",
    "#       print(class_correct, class_samples)\n",
    "#       print([class_correct[idx]/class_samples[idx] for idx in range(2)])\n",
    "#       return test_acc\n",
    "\n",
    "# def masked_test():\n",
    "#       class_correct = [0,0]\n",
    "#       class_samples = [0,0]\n",
    "#       model.eval()\n",
    "#       out = model(masked_data.x)\n",
    "#       pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#       test_correct = pred[masked_data.test_mask] == masked_data.y[masked_data.test_mask]  # Check against ground-truth labels.\n",
    "#       assert len(test_correct)==len(pred[masked_data.test_mask])\n",
    "#       #print(test_correct)\n",
    "#       test_acc = int(test_correct.sum()) / int(masked_data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "#       p = pred[masked_data.test_mask]\n",
    "#       d = data.y[masked_data.test_mask]      \n",
    "#       for idx,each in enumerate(test_correct):\n",
    "#           class_samples[d[idx]] += 1\n",
    "#           if each == True:\n",
    "#               class_correct[p[idx]] += 1\n",
    "#       print(class_correct, class_samples)\n",
    "#       print([class_correct[idx]/class_samples[idx] for idx in range(2)])\n",
    "#       return test_acc\n",
    "\n",
    "# for epoch in range(1, 101):\n",
    "#     loss = train()\n",
    "#     print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "# test_acc = test()\n",
    "# print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# # if MASK:\n",
    "# #     masked_test_acc = masked_test()\n",
    "# #     print(f'Masked Test Accuracy: {masked_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        #torch.manual_seed(12345)\n",
    "        #print(len(set(data.y)))\n",
    "        self.conv1 = GCNConv(len(data.x[0]), hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        x = self.conv1(x, edge_index, edge_weight=edge_weight)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        return x\n",
    "\n",
    "class GatV2ConvClass(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, heads=4, edge_dim=1):\n",
    "        super().__init__()\n",
    "        self.edge_dim=edge_dim\n",
    "        #torch.manual_seed(12345)\n",
    "        #print(len(set(data.y)))\n",
    "        self.conv1 = GATv2Conv(len(data.x[0]), hidden_channels, heads=heads)\n",
    "        self.conv2 = GATv2Conv(hidden_channels*heads, 2)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, size=None):\n",
    "        x = self.conv1(x, edge_index, edge_attr, size)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index,edge_attr, size)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SAGEConvClass(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels,dropout=0.5,edge_dim=1):\n",
    "        super().__init__()\n",
    "        self.edge_dim=edge_dim\n",
    "        #torch.manual_seed(12345)\n",
    "        #print(len(set(data.y)))\n",
    "        self.conv1 = SAGEConv(len(data.x[0]), hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, 2)\n",
    "        self.dropout_prob = dropout\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=self.dropout_prob, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net()\n",
    "# model.eval()\n",
    "# out = model(data.x, data.edge_index, data.edge_attr)\n",
    "# visualize(out, color=data.y)\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GCN(hidden_channels=16)\n",
    "# model.eval()\n",
    "\n",
    "# print(data.edge_weight.size(), data.edge_index.size())\n",
    "# out = model(data.x, data.edge_index, data.edge_weight)\n",
    "# visualize(out, color=data.y)\n",
    "# print(model)\n",
    "# model = GCN(hidden_channels=16)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GatV2ConvClass(hidden_channels=16)\n",
    "# model.eval()\n",
    "# out = model(data.x, data.edge_index)#, data.edge_attr)#, len(data.edge_attr))\n",
    "# visualize(out, color=data.y)\n",
    "# print(model)\n",
    "# model = GatV2ConvClass(hidden_channels=16)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = SAGEConvClass(hidden_channels=16)\n",
    "# model.eval()\n",
    "# out = model(data.x, data.edge_index)#, data.edge_attr)#, len(data.edge_attr))\n",
    "# visualize(out, color=data.y)\n",
    "# print(model)\n",
    "# data = data.to(device)\n",
    "# model = SAGEConvClass(hidden_channels=16).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train():\n",
    "#       model.train()\n",
    "#       optimizer.zero_grad()  # Clear gradients.\n",
    "#       out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "#       loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "#       loss.backward()  # Derive gradients.\n",
    "#       optimizer.step()  # Update parameters based on gradients.\n",
    "#       return loss\n",
    "\n",
    "\n",
    "# def validate():\n",
    "#       model.eval()\n",
    "#       out = model(data.x, data.edge_index)\n",
    "#       pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#       test_correct = pred[data.val_mask] == data.y[data.val_mask]  # Check against ground-truth labels.\n",
    "#       test_acc = int(test_correct.sum()) / int(data.val_mask.sum())  # Derive ratio of correct predictions.\n",
    "#       return test_acc\n",
    "\n",
    "# def test():\n",
    "#       class_correct = [0,0]\n",
    "#       class_samples = [0,0]\n",
    "#       model.eval()\n",
    "#       out = model(data.x, data.edge_index)\n",
    "#       pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "#       test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "#       assert len(test_correct)==len(pred[data.test_mask])\n",
    "#       #print(test_correct)\n",
    "#       test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "#       p = pred[data.test_mask]\n",
    "#       d = data.y[data.test_mask]\n",
    "      \n",
    "#       for idx,each in enumerate(test_correct):          \n",
    "#           class_samples[p[idx]] += 1\n",
    "#           if each == True:\n",
    "#               class_correct[p[idx]] += 1\n",
    "#       print(class_correct, class_samples)\n",
    "#       print([class_correct[idx]/class_samples[idx] for idx in range(2)])\n",
    "#       return test_acc\n",
    "\n",
    "# model = SAGEConvClass(hidden_channels=16).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# for epoch in range(1, 51):\n",
    "#       loss = train()\n",
    "#       print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')\n",
    "#       if (epoch) % 25 == 0:\n",
    "#             val_acc =validate()\n",
    "#             print(f'Validate Accuracy: {val_acc:.4f}')\n",
    "# test_acc = test()\n",
    "# print(f'Test Accuracy: {test_acc:.4f}')\n",
    "\n",
    "# model.eval()\n",
    "# out = model(data.x, data.edge_index)\n",
    "# visualize(out.detach().cpu(), color=data.y.detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "if LOAD_TEST_SET and not USE_BRANDS_AND_INGR:\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  data = data.to(device)\n",
    "  # WEIGHTS = torch.tensor([1-proportion, proportion])\n",
    "  # WEIGHTS = WEIGHTS.to(device)\n",
    "  def gridsearch(model_path, hidden_channels, lrs,weight_decays, save_path, dropouts):\n",
    "      def test():\n",
    "          class_correct = [0,0]\n",
    "          class_samples = [0,0]\n",
    "          model.eval()\n",
    "          out = model(data.x, data.edge_index)\n",
    "          pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "          test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "          assert len(test_correct)==len(pred[data.test_mask])\n",
    "          #print(test_correct)\n",
    "          sm = torch.nn.Softmax(dim=1)\n",
    "          probs__ = sm(out)\n",
    "          greater_class_probs = probs__[:,1]\n",
    "          test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "          p = pred[data.test_mask]\n",
    "          d = data.y[data.test_mask]\n",
    "          \n",
    "          d_ = d.cpu().numpy()\n",
    "          p_ = p.cpu().numpy()\n",
    "          greater_class_probs_ = greater_class_probs[data.test_mask].cpu().detach().numpy()\n",
    "\n",
    "          precision = precision_score(d_, p_)\n",
    "          recall = recall_score(d_, p_)\n",
    "          f1 = f1_score(y_true = d_, y_pred=p_)\n",
    "          return roc_auc_score(d_,greater_class_probs[data.test_mask].cpu().detach().numpy()),test_acc, precision,recall,f1\n",
    "\n",
    "      num_features = len(data.x[0])\n",
    "      stop_at = 0\n",
    "      data_dict = {}\n",
    "      end_str = INGREDIENT_TO_CLASSIFY\n",
    "      data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{},'Validation AUC':{},\\\n",
    "        'Validation Precision':{},'Validation Recall': {}, 'Validation F1':{}, 'Test Precision':{},'Test Recall': {}, 'Test F1':{}}\n",
    "      gridsearch_ = {}\n",
    "      for hidden_chans in hidden_channels:\n",
    "          for lr in lrs:\n",
    "            for wd in weight_decays:\n",
    "              for do in dropouts:\n",
    "                for i in range(25):\n",
    "                  model = SAGEConvClass(hidden_chans, dropout=do).to(device)\n",
    "                  model.load_state_dict(torch.load(f'{model_path}_{i}'))\n",
    "                  model.eval()\n",
    "                  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                  criterion = torch.nn.CrossEntropyLoss()\n",
    "                  max_acc = 0.0\n",
    "                  max_auc = 0.0\n",
    "                  max_test_auc = 0.0\n",
    "                  max_test_acc = 0.0\n",
    "                  test_acc_li = []\n",
    "                  test_auc_li = []\n",
    "                  test_prec_li = []\n",
    "                  test_rec_li = []\n",
    "                  test_f1_li = []\n",
    "                  test_auc,test_acc, test_precision, test_recall, test_f1 = test()\n",
    "                  test_auc_li.append(round(test_auc,3))\n",
    "                  test_acc_li.append(round(test_acc,3))\n",
    "                  test_prec_li.append(test_precision)\n",
    "                  test_rec_li.append(test_recall)\n",
    "                  test_f1_li.append(test_f1)\n",
    "                  max_test_auc = test_auc\n",
    "                  max_test_acc = test_acc\n",
    "                  print(f'HC: {hidden_channels} | LR: {lr} | WD: {wd} | DrpOut: {do} | Run: {i} | \\\n",
    "                    Accuracy: {test_acc} | AUC: {test_auc} | Precision: {test_precision} | Recall: {test_recall} | F1: {test_f1_li}')\n",
    "    \n",
    "                  data_dict[end_str]['Test Accuracy'][i]= test_acc_li\n",
    "                  data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "                  data_dict[end_str]['Test Precision'][i] = test_prec_li\n",
    "                  data_dict[end_str]['Test Recall'][i] = test_rec_li\n",
    "                  data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "            with open(save_path+f'{INGREDIENT_TO_CLASSIFY}_graphsage_data_dict_TESTING', 'wb') as f:\n",
    "              pickle.dump(data_dict, f)\n",
    "    \n",
    "  # 'Hids: 128 lr: 0.01 wd: 0.005 dropout: 0.5'\n",
    "  hidden_chans = [128]#64 lr: 0.01 wd: 0.005\n",
    "  lrs = [0.01]\n",
    "  wds = [0.005]\n",
    "  dropouts = [0.5]\n",
    "  save_path = 'gridsearch/node/'\n",
    "  model_path = f'{INGREDIENT_TO_CLASSIFY}_DEMO_TEST_GNN'\n",
    "  gridsearch(model_path,hidden_chans,lrs,wds, save_path,dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_BRANDS_AND_INGR and not LOAD_TEST_SET:\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  data = data.to(device)\n",
    "  # WEIGHTS = torch.tensor([1-proportion, proportion])\n",
    "  # WEIGHTS = WEIGHTS.to(device)\n",
    "  def gridsearch(hidden_channels, lrs,weight_decays, save_path, dropouts):\n",
    "      def train(optimizer, criterion):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data.x, data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "        # loss = (loss*WEIGHTS).mean()\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        return loss\n",
    "\n",
    "\n",
    "      def validate():\n",
    "          model.eval()\n",
    "          out = model(data.x, data.edge_index)\n",
    "          pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "          sm = torch.nn.Softmax(dim=1)\n",
    "          probs__ = sm(out)\n",
    "          greater_class_probs = probs__[:,1]\n",
    "\n",
    "          test_correct = pred[data.val_mask] == data.y[data.val_mask]  # Check against ground-truth labels.\n",
    "          test_acc = int(test_correct.sum()) / int(data.val_mask.sum())  # Derive ratio of correct predictions.\n",
    "\n",
    "          d_ = data.y[data.val_mask].cpu().numpy()\n",
    "          p_ = pred[data.val_mask].cpu().numpy()\n",
    "\n",
    "          #Precision, Recall and F1 Score\n",
    "          precision = precision_score(d_, p_)\n",
    "          recall = recall_score(d_, p_)\n",
    "          f1 = f1_score(y_true = d_, y_pred=p_)\n",
    "\n",
    "          return roc_auc_score(d_,greater_class_probs[data.val_mask].cpu().detach().numpy()),test_acc, precision,recall,f1\n",
    "      \n",
    "      \n",
    "      def test():\n",
    "          class_correct = [0,0]\n",
    "          class_samples = [0,0]\n",
    "          model.eval()\n",
    "          out = model(data.x, data.edge_index)\n",
    "          pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "          test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "          assert len(test_correct)==len(pred[data.test_mask])\n",
    "          #print(test_correct)\n",
    "          sm = torch.nn.Softmax(dim=1)\n",
    "          probs__ = sm(out)\n",
    "          greater_class_probs = probs__[:,1]\n",
    "          test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "          p = pred[data.test_mask]\n",
    "          d = data.y[data.test_mask]\n",
    "          \n",
    "          d_ = d.cpu().numpy()\n",
    "          p_ = p.cpu().numpy()\n",
    "          greater_class_probs_ = greater_class_probs[data.test_mask].cpu().detach().numpy()\n",
    "\n",
    "          precision = precision_score(d_, p_)\n",
    "          recall = recall_score(d_, p_)\n",
    "          f1 = f1_score(y_true = d_, y_pred=p_)\n",
    "          return roc_auc_score(d_,greater_class_probs[data.test_mask].cpu().detach().numpy()),test_acc, precision,recall,f1\n",
    "\n",
    "          for idx,each in enumerate(test_correct):          \n",
    "              class_samples[d[idx]] += 1\n",
    "              if each == True:\n",
    "                  class_correct[p[idx]] += 1\n",
    "          # print(class_correct, class_samples)\n",
    "          # print([class_correct[idx]/class_samples[idx] for idx in range(2)])\n",
    "          return roc_auc_score(d.cpu().numpy(),greater_class_probs_),test_acc, precision, recall, f1\n",
    "\n",
    "      num_features = len(data.x[0])\n",
    "      stop_at = 0\n",
    "      data_dict = {}\n",
    "      end_str = INGREDIENT_TO_CLASSIFY\n",
    "      data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{},'Validation AUC':{},\\\n",
    "        'Validation Precision':{},'Validation Recall': {}, 'Validation F1':{}, 'Test Precision':{},'Test Recall': {}, 'Test F1':{}}\n",
    "      gridsearch_ = {}\n",
    "      for hidden_chans in hidden_channels:\n",
    "          for lr in lrs:\n",
    "            for wd in weight_decays:\n",
    "              for do in dropouts:\n",
    "                for i in range(25):\n",
    "                  model = SAGEConvClass(hidden_chans, dropout=do).to(device)\n",
    "                  optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                  criterion = torch.nn.CrossEntropyLoss()\n",
    "                  max_acc = 0.0\n",
    "                  max_auc = 0.0\n",
    "                  max_val = 0.0\n",
    "                  max_test_auc = 0.0\n",
    "                  max_test_acc = 0.0\n",
    "                  acc_li = []\n",
    "                  auc_li = []\n",
    "                  test_acc_li = []\n",
    "                  test_auc_li = []\n",
    "                  val_prec_li = []\n",
    "                  val_rec_li = []\n",
    "                  val_f1_li = []\n",
    "                  test_prec_li = []\n",
    "                  test_rec_li = []\n",
    "                  test_f1_li = []\n",
    "                  loss_li = []\n",
    "                  for epoch in range(1,401):\n",
    "                    loss=train(optimizer,criterion)\n",
    "                    loss_li.append(loss.cpu().detach().item())\n",
    "                    if (epoch%10)==0 or epoch==1:\n",
    "                      val_auc,val_acc, val_precision, val_recall, val_f1 =validate()\n",
    "                      val_auc = round(val_auc, 3)\n",
    "                      val_acc = round(val_acc, 3)\n",
    "                      acc_li.append(val_acc)\n",
    "                      auc_li.append(val_auc)\n",
    "                      val_prec_li.append(val_precision)\n",
    "                      val_rec_li.append(val_recall)\n",
    "                      val_f1_li.append(val_f1)\n",
    "\n",
    "                      test_auc,test_acc, test_precision, test_recall, test_f1 = test()\n",
    "                      test_auc_li.append(round(test_auc,3))\n",
    "                      test_acc_li.append(round(test_acc,3))\n",
    "                      test_prec_li.append(test_precision)\n",
    "                      test_rec_li.append(test_recall)\n",
    "                      test_f1_li.append(test_f1)\n",
    "                      if val_acc >= max_val :\n",
    "                        stop_at = epoch\n",
    "                        max_auc = val_auc \n",
    "                        max_val = val_acc \n",
    "                        max_test_auc = test_auc\n",
    "                        max_test_acc = test_acc\n",
    "                        torch.save(model.state_dict(), f'{INGREDIENT_TO_CLASSIFY}_DEMO_TEST_GNN_{i}')\n",
    "                  print(f'HC: {hidden_channels} | LR: {lr} | WD: {wd} | DrpOut: {do} | Run: {i} | Accuracy: {val_acc} | AUC: {val_auc} | Precision: {val_precision} | Recall: {val_recall} | F1: {val_f1}')\n",
    "                      #print(f'Epoch: {epoch} | Accuracy: {val_acc} | AUC: {val_auc} | Precision: {val_precision} | Recall: {val_recall} | F1: {val_f1}')\n",
    "\n",
    "                        # torch.save(model.state_dict(), save_path+f'mlp_state_dict')\n",
    "                  # if max_test_auc > 0.9:\n",
    "                  #   print(f'Hids: {hidden_chans}',f'lr: {lr}',f'wd: {wd}',f'dropout: {do}',f'Max Test Accuracy: {max_test_acc:.4f} Max Test AUC: {max_test_auc:.4f}')\n",
    "                  # string_ = f'Hids: {hidden_chans} lr: {lr} wd: {wd} dropout: {do}'\n",
    "                  # gridsearch_[string_] = {}\n",
    "                  # gridsearch_[string_]['ACC'] = max_test_acc\n",
    "                  # gridsearch_[string_]['AUC'] = max_test_auc\n",
    "                  # with open(f'{INGREDIENT_TO_CLASSIFY}_gridsearch', 'wb') as f:\n",
    "                  #   pickle.dump(gridsearch_, f)\n",
    "\n",
    "                  data_dict[end_str]['train_loss'][i] = loss_li\n",
    "                  data_dict[end_str]['Validation Accuracy'][i] = acc_li\n",
    "                  data_dict[end_str]['Validation AUC'][i] = auc_li\n",
    "                  data_dict[end_str]['Validation Precision'][i] = val_prec_li\n",
    "                  data_dict[end_str]['Validation Recall'][i] = val_rec_li\n",
    "                  data_dict[end_str]['Validation F1'][i] = val_f1_li\n",
    "                  data_dict[end_str]['Test Accuracy'][i]= test_acc_li\n",
    "                  data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "                  data_dict[end_str]['Test Precision'][i] = test_prec_li\n",
    "                  data_dict[end_str]['Test Recall'][i] = test_rec_li\n",
    "                  data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "            with open(save_path+f'{INGREDIENT_TO_CLASSIFY}_graphsage_data_dict_{stop_at}', 'wb') as f:\n",
    "              pickle.dump(data_dict, f)\n",
    "\n",
    "              \n",
    "  # 'Hids: 128 lr: 0.01 wd: 0.005 dropout: 0.5'\n",
    "  hidden_chans = [128]#64 lr: 0.01 wd: 0.005\n",
    "  lrs = [0.01]\n",
    "  wds = [0.005]\n",
    "  dropouts = [0.5]\n",
    "  save_path = 'gridsearch/node/'\n",
    "  gridsearch(hidden_chans,lrs,wds, save_path,dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 0 | Accuracy: 0.8733435270132518 | AUC: 0.9164118009046024 | Precision: 0.6869070208728653 | Recall: 0.8125701459034792 | F1: 0.7444730077120821\n",
      "Run: 1 | Accuracy: 0.867737003058104 | AUC: 0.9162413970085143 | Precision: 0.6684782608695652 | Recall: 0.8282828282828283 | F1: 0.7398496240601504\n",
      "Run: 2 | Accuracy: 0.8595820591233435 | AUC: 0.915769964731389 | Precision: 0.6478260869565218 | Recall: 0.8361391694725028 | F1: 0.7300342969132777\n",
      "Run: 3 | Accuracy: 0.8634046890927625 | AUC: 0.9168750922789828 | Precision: 0.6569407603890363 | Recall: 0.8338945005611672 | F1: 0.734915924826904\n",
      "Run: 4 | Accuracy: 0.8705402650356778 | AUC: 0.916444919577132 | Precision: 0.6788048552754435 | Recall: 0.8159371492704826 | F1: 0.7410805300713558\n",
      "Run: 5 | Accuracy: 0.867737003058104 | AUC: 0.9149499538003769 | Precision: 0.6715867158671587 | Recall: 0.8170594837261503 | F1: 0.7372151898734176\n",
      "Run: 6 | Accuracy: 0.8651885830784913 | AUC: 0.9162710002912222 | Precision: 0.661319073083779 | Recall: 0.8327721661054994 | F1: 0.7372081470442126\n",
      "Run: 7 | Accuracy: 0.872579001019368 | AUC: 0.9163403829850693 | Precision: 0.6853080568720379 | Recall: 0.8114478114478114 | F1: 0.7430626927029805\n",
      "Run: 8 | Accuracy: 0.863914373088685 | AUC: 0.9169805539736301 | Precision: 0.6578249336870027 | Recall: 0.835016835016835 | F1: 0.7359050445103857\n",
      "Run: 9 | Accuracy: 0.8662079510703364 | AUC: 0.9160521210196998 | Precision: 0.6639784946236559 | Recall: 0.8316498316498316 | F1: 0.7384155455904334\n",
      "Run: 10 | Accuracy: 0.8687563710499491 | AUC: 0.9151447804046989 | Precision: 0.6721611721611722 | Recall: 0.8237934904601572 | F1: 0.7402924861321231\n",
      "Run: 11 | Accuracy: 0.8641692150866462 | AUC: 0.9175580030069534 | Precision: 0.6570175438596492 | Recall: 0.8406285072951739 | F1: 0.7375677006400788\n",
      "Run: 12 | Accuracy: 0.8636595310907238 | AUC: 0.9175065673032482 | Precision: 0.6566901408450704 | Recall: 0.8372615039281706 | F1: 0.7360631475086333\n",
      "Run: 13 | Accuracy: 0.8616207951070336 | AUC: 0.9162933877737702 | Precision: 0.6505190311418685 | Recall: 0.8439955106621774 | F1: 0.734733756717147\n",
      "Run: 14 | Accuracy: 0.8616207951070336 | AUC: 0.9156102920252827 | Precision: 0.6534391534391535 | Recall: 0.8316498316498316 | F1: 0.7318518518518518\n",
      "Run: 15 | Accuracy: 0.8672273190621814 | AUC: 0.9175975973975754 | Precision: 0.667572463768116 | Recall: 0.8271604938271605 | F1: 0.7388471177944863\n",
      "Run: 16 | Accuracy: 0.8644240570846076 | AUC: 0.9166121781244322 | Precision: 0.6642268984446478 | Recall: 0.8148148148148148 | F1: 0.7318548387096774\n",
      "Run: 17 | Accuracy: 0.8654434250764526 | AUC: 0.9176229452083942 | Precision: 0.6619090098126673 | Recall: 0.8327721661054994 | F1: 0.7375745526838966\n",
      "Run: 18 | Accuracy: 0.863914373088685 | AUC: 0.9153588491427813 | Precision: 0.6606660666066607 | Recall: 0.8237934904601572 | F1: 0.7332667332667334\n",
      "Run: 19 | Accuracy: 0.8713047910295617 | AUC: 0.9157777355930999 | Precision: 0.6797020484171322 | Recall: 0.819304152637486 | F1: 0.7430025445292621\n",
      "Run: 20 | Accuracy: 0.8651885830784913 | AUC: 0.9167844322256895 | Precision: 0.6621863799283154 | Recall: 0.8294051627384961 | F1: 0.7364225211758844\n",
      "Run: 21 | Accuracy: 0.8664627930682977 | AUC: 0.9160748785432816 | Precision: 0.6642793196060878 | Recall: 0.8327721661054994 | F1: 0.7390438247011951\n",
      "Run: 22 | Accuracy: 0.8618756371049949 | AUC: 0.9186444434823379 | Precision: 0.652668416447944 | Recall: 0.8372615039281706 | F1: 0.7335299901671584\n",
      "Run: 23 | Accuracy: 0.8761467889908257 | AUC: 0.9172812123136335 | Precision: 0.6967930029154519 | Recall: 0.8047138047138047 | F1: 0.746875\n",
      "Run: 24 | Accuracy: 0.867737003058104 | AUC: 0.9171394865976688 | Precision: 0.6681735985533453 | Recall: 0.8294051627384961 | F1: 0.7401101652478718\n"
     ]
    }
   ],
   "source": [
    "if LOAD_TEST_SET and USE_BRANDS_AND_INGR:\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  data = data.to(device)\n",
    "  def gridsearch(model_path, hidden_channels, lrs,weight_decays, save_path):  \n",
    "      def test():\n",
    "          class_correct = [0,0]\n",
    "          class_samples = [0,0]\n",
    "          model.eval()\n",
    "          out = model(data.x)\n",
    "          pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "          test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "          assert len(test_correct)==len(pred[data.test_mask])\n",
    "          #print(test_correct)\n",
    "          test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "          p = pred[data.test_mask]\n",
    "          d = data.y[data.test_mask]\n",
    "          sm = torch.nn.Softmax(dim=1)\n",
    "          probs__ = sm(out)\n",
    "          greater_class_probs = probs__[:,1]\n",
    "          \n",
    "          for idx,each in enumerate(test_correct):          \n",
    "              class_samples[d[idx]] += 1\n",
    "              if each == True:\n",
    "                  class_correct[p[idx]] += 1\n",
    "          # print(class_correct, class_samples)\n",
    "          # print([class_correct[idx]/class_samples[idx] for idx in range(2)])\n",
    "\n",
    "          d_ = data.y[data.test_mask].cpu().numpy()\n",
    "          p_ = pred[data.test_mask].cpu().numpy()\n",
    "\n",
    "          precision = precision_score(d_, p_)\n",
    "          recall = recall_score(d_, p_)\n",
    "          f1 = f1_score(d_, p_)\n",
    "\n",
    "          return roc_auc_score(d_,greater_class_probs[data.test_mask].cpu().detach().numpy()),test_acc, precision,recall,f1\n",
    "\n",
    "      num_features = len(data.x[0])\n",
    "      data_dict = {}\n",
    "      end_str = INGREDIENT_TO_CLASSIFY\n",
    "      stop_at = 0\n",
    "      data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{},'Validation AUC':{},\\\n",
    "        'Validation Precision':{},'Validation Recall': {}, 'Validation F1':{}, 'Test Precision':{},'Test Recall': {}, 'Test F1':{}}\n",
    "      for hidden_chans in hidden_channels:\n",
    "          for lr in lrs:\n",
    "            for wd in weight_decays:\n",
    "              for i in range(25):\n",
    "                model = MLP(hidden_chans)\n",
    "                model.load_state_dict(torch.load(f'{model_path}_{i}'))\n",
    "                model = model.to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                max_acc = 0.0\n",
    "                max_auc = 0.0\n",
    "\n",
    "                test_acc_li = []\n",
    "                test_auc_li = []\n",
    "                test_prec_li = []\n",
    "                test_rec_li = []\n",
    "                test_f1_li = []\n",
    "\n",
    "                test_auc,test_acc, test_precision, test_recall, test_f1 = test()\n",
    "                test_auc_li.append(round(test_auc,3))\n",
    "                test_acc_li.append(round(test_acc,3))\n",
    "                test_prec_li.append(test_precision)\n",
    "                test_rec_li.append(test_recall)\n",
    "                test_f1_li.append(test_f1)\n",
    "\n",
    "\n",
    " \n",
    "                # torch.save(model.state_dict(), save_path+f'mlp_state_dict')\n",
    "                print(f'Run: {i} | Accuracy: {test_acc} | AUC: {test_auc} | Precision: {test_precision} | Recall: {test_recall} | F1: {test_f1}')\n",
    "\n",
    "                #print(f'Hids: {hidden_chans}',f'lr: {lr}',f'wd: {wd}',f'Max Validate Accuracy: {max_val:.4f} Max Validate AUC: {max_auc:.4f}')\n",
    "                \n",
    "                data_dict[end_str]['Test Accuracy'][i]= test_acc_li\n",
    "                data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "                data_dict[end_str]['Test Precision'][i] = test_prec_li\n",
    "                data_dict[end_str]['Test Recall'][i] = test_rec_li\n",
    "                data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "            with open(save_path+f'{INGREDIENT_TO_CLASSIFY}_mlp_data_dict_TESTING', 'wb') as f:\n",
    "              pickle.dump(data_dict, f)\n",
    "\n",
    "  hidden_chans = [128]#64 lr: 0.01 wd: 0.005\n",
    "  lrs = [0.01]\n",
    "  wds = [0.005]\n",
    "  dropouts = [0.5]\n",
    "  save_path = 'gridsearch/node/'\n",
    "  model_path = f'{INGREDIENT_TO_CLASSIFY}_DEMO_TEST_MLP'\n",
    "  gridsearch(model_path,hidden_chans,lrs,wds, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_BRANDS_AND_INGR and not LOAD_TEST_SET:\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "  data = data.to(device)\n",
    "  def gridsearch(hidden_channels, lrs,weight_decays, save_path):\n",
    "      def train(optimizer, criterion):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(data.x)  # Perform a single forward pass.\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        return loss\n",
    "\n",
    "\n",
    "      def validate():\n",
    "          model.eval()\n",
    "          out = model(data.x)\n",
    "          pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "          test_correct = pred[data.val_mask] == data.y[data.val_mask]  # Check against ground-truth labels.\n",
    "          test_acc = int(test_correct.sum()) / int(data.val_mask.sum())  # Derive ratio of correct predictions.]\n",
    "          \n",
    "          sm = torch.nn.Softmax(dim=1)\n",
    "          probs__ = sm(out)\n",
    "          greater_class_probs = probs__[:,1]\n",
    "\n",
    "          d_ = data.y[data.val_mask].cpu().numpy()\n",
    "          p_ = pred[data.val_mask].cpu().numpy()\n",
    "\n",
    "          precision = precision_score(d_, p_)\n",
    "          recall = recall_score(d_, p_)\n",
    "          f1 = f1_score(d_, p_)\n",
    "\n",
    "          return roc_auc_score(d_,greater_class_probs[data.val_mask].cpu().detach().numpy()),test_acc, precision,recall,f1\n",
    "      \n",
    "      def test():\n",
    "          class_correct = [0,0]\n",
    "          class_samples = [0,0]\n",
    "          model.eval()\n",
    "          out = model(data.x)\n",
    "          pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "          test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "          assert len(test_correct)==len(pred[data.test_mask])\n",
    "          #print(test_correct)\n",
    "          test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "          p = pred[data.test_mask]\n",
    "          d = data.y[data.test_mask]\n",
    "          sm = torch.nn.Softmax(dim=1)\n",
    "          probs__ = sm(out)\n",
    "          greater_class_probs = probs__[:,1]\n",
    "          \n",
    "          for idx,each in enumerate(test_correct):          \n",
    "              class_samples[d[idx]] += 1\n",
    "              if each == True:\n",
    "                  class_correct[p[idx]] += 1\n",
    "          # print(class_correct, class_samples)\n",
    "          # print([class_correct[idx]/class_samples[idx] for idx in range(2)])\n",
    "\n",
    "          d_ = data.y[data.test_mask].cpu().numpy()\n",
    "          p_ = pred[data.test_mask].cpu().numpy()\n",
    "\n",
    "          precision = precision_score(d_, p_)\n",
    "          recall = recall_score(d_, p_)\n",
    "          f1 = f1_score(d_, p_)\n",
    "\n",
    "          return roc_auc_score(d_,greater_class_probs[data.test_mask].cpu().detach().numpy()),test_acc, precision,recall,f1\n",
    "\n",
    "      num_features = len(data.x[0])\n",
    "      data_dict = {}\n",
    "      end_str = INGREDIENT_TO_CLASSIFY\n",
    "      stop_at = 0\n",
    "      data_dict[end_str] = {'train_loss':{}, 'Test Accuracy':{},'Test AUC':{}, 'Validation Accuracy':{},'Validation AUC':{},\\\n",
    "        'Validation Precision':{},'Validation Recall': {}, 'Validation F1':{}, 'Test Precision':{},'Test Recall': {}, 'Test F1':{}}\n",
    "      for hidden_chans in hidden_channels:\n",
    "          for lr in lrs:\n",
    "            for wd in weight_decays:\n",
    "              for i in range(25):\n",
    "                model = MLP(hidden_chans).to(device)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                max_acc = 0.0\n",
    "                max_auc = 0.0\n",
    "                max_val = 0.0\n",
    "                acc_li = []\n",
    "                auc_li = []\n",
    "                test_acc_li = []\n",
    "                test_auc_li = []\n",
    "                val_prec_li = []\n",
    "                val_rec_li = []\n",
    "                val_f1_li = []\n",
    "                test_prec_li = []\n",
    "                test_rec_li = []\n",
    "                test_f1_li = []\n",
    "                loss_li = []\n",
    "                for epoch in range(1,401):\n",
    "                  loss=train(optimizer,criterion)\n",
    "                  loss_li.append(loss.cpu().detach().item())\n",
    "                  if (epoch%10)==0 or epoch==1:\n",
    "                    val_auc,val_acc, val_precision, val_recall, val_f1 =validate()\n",
    "                    val_auc = round(val_auc, 3)\n",
    "                    val_acc = round(val_acc, 3)\n",
    "                    acc_li.append(val_acc)\n",
    "                    auc_li.append(val_auc)\n",
    "                    val_prec_li.append(val_precision)\n",
    "                    val_rec_li.append(val_recall)\n",
    "                    val_f1_li.append(val_f1)\n",
    "\n",
    "                    test_auc,test_acc, test_precision, test_recall, test_f1 = test()\n",
    "                    test_auc_li.append(round(test_auc,3))\n",
    "                    test_acc_li.append(round(test_acc,3))\n",
    "                    test_prec_li.append(test_precision)\n",
    "                    test_rec_li.append(test_recall)\n",
    "                    test_f1_li.append(test_f1)\n",
    "\n",
    "                    if val_auc >= max_auc :\n",
    "                      stop_at = epoch\n",
    "                      max_auc = val_auc \n",
    "                      max_val = val_acc \n",
    "                      torch.save(model.state_dict(), f'{INGREDIENT_TO_CLASSIFY}_DEMO_TEST_MLP_{i}')\n",
    "                      #print(f'Epoch: {epoch} | Accuracy: {val_acc} | AUC: {val_auc} | Precision: {val_precision} | Recall: {val_recall} | F1: {val_f1}')\n",
    "\n",
    "                print(f'Hids: {hidden_chans}',f'lr: {lr}',f'wd: {wd}',f'Max Validate Accuracy: {max_val:.4f} Max Validate AUC: {max_auc:.4f}')\n",
    "                \n",
    "                \n",
    "                data_dict[end_str]['train_loss'][i] = loss_li\n",
    "                data_dict[end_str]['Validation Accuracy'][i] = acc_li\n",
    "                data_dict[end_str]['Validation AUC'][i] = auc_li\n",
    "                data_dict[end_str]['Validation Precision'][i] = val_prec_li\n",
    "                data_dict[end_str]['Validation Recall'][i] = val_rec_li\n",
    "                data_dict[end_str]['Validation F1'][i] = val_f1_li\n",
    "                data_dict[end_str]['Test Accuracy'][i]= test_acc_li\n",
    "                data_dict[end_str]['Test AUC'][i] = test_auc_li\n",
    "                data_dict[end_str]['Test Precision'][i] = test_prec_li\n",
    "                data_dict[end_str]['Test Recall'][i] = test_rec_li\n",
    "                data_dict[end_str]['Test F1'][i] = test_f1_li\n",
    "            with open(save_path+f'{INGREDIENT_TO_CLASSIFY}_mlp_data_dict_stop_epoch_{stop_at}', 'wb') as f:\n",
    "              pickle.dump(data_dict, f)\n",
    "  hidden_chans = [128]#64 lr: 0.01 wd: 0.005\n",
    "  lrs = [0.01]\n",
    "  wds = [0.005]\n",
    "  dropouts = [0.5]\n",
    "  save_path = 'gridsearch/node/'\n",
    "  gridsearch(hidden_chans,lrs,wds, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4227, 10605])\n",
      "Decision Tree Validation Acc 0.8340943683409436 and Validation AUC 0.8259514170040486\n",
      "Grad Boost Validation Acc 0.8462709284627092 and Validation AUC 0.8798785425101215\n",
      "Decision Tree test Acc 0.8340943683409436 and test AUC 0.8259514170040486 and Test F1: 0.7009803921568627\n",
      "Grad Boost test Acc 0.8358662613981763 and test AUC 0.8886453839516826 and Test F1: 0.7202072538860103\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree,ensemble\n",
    "from joblib import dump, load\n",
    "\n",
    "if not MASK and USE_BRANDS_AND_INGR:\n",
    "    assert len(data.y) == len(data.test_mask)==len(data.x)\n",
    "    data.detach().cpu()\n",
    "    print(data.x.shape)\n",
    "    if LOAD_TEST_SET:\n",
    "        clf = load(f'{INGREDIENT_TO_CLASSIFY}_Decision_Tree')\n",
    "        grad_boost = load(f'{INGREDIENT_TO_CLASSIFY}_Grad_Boost')\n",
    "    else:\n",
    "        clf = tree.DecisionTreeClassifier()\n",
    "        grad_boost = ensemble.GradientBoostingClassifier()\n",
    "\n",
    "        clf.fit(data.x[data.train_mask], data.y[data.train_mask])\n",
    "        grad_boost.fit(data.x[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "\n",
    "    clf_preds = torch.Tensor(clf.predict(data.x[data.val_mask]))\n",
    "    clf_val_acc = int((clf_preds==data.y[data.val_mask]).sum()) / int(data.val_mask.sum())\n",
    "    grad_val_acc = grad_boost.score(data.x[data.val_mask], data.y[data.val_mask])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    clf_val_probas = clf.predict_proba(data.x[data.val_mask])\n",
    "    grad_val_probas = grad_boost.predict_proba(data.x[data.val_mask])\n",
    "\n",
    "\n",
    "    clf_val_auc = roc_auc_score(data.y[data.val_mask],clf_val_probas[:,1])\n",
    "    grad_val_auc = roc_auc_score(data.y[data.val_mask],grad_val_probas[:,1])\n",
    "\n",
    "    print(f'Decision Tree Validation Acc {clf_val_acc} and Validation AUC {clf_val_auc}')\n",
    "    print(f'Grad Boost Validation Acc {grad_val_acc} and Validation AUC {grad_val_auc}')\n",
    "\n",
    "\n",
    "    clf_preds = torch.Tensor(clf.predict(data.x[data.test_mask]))\n",
    "\n",
    "    clf_test_acc = int((clf_preds==data.y[data.test_mask]).sum()) / int(data.test_mask.sum())\n",
    "    grad_test_acc = grad_boost.score(data.x[data.test_mask], data.y[data.test_mask])\n",
    "\n",
    "\n",
    "    clf_test_probas = clf.predict_proba(data.x[data.test_mask])\n",
    "    grad_test_probas = grad_boost.predict_proba(data.x[data.test_mask])\n",
    "\n",
    "\n",
    "    clf_test_auc = roc_auc_score(data.y[data.test_mask],clf_test_probas[:,1])\n",
    "    grad_test_auc = roc_auc_score(data.y[data.test_mask],grad_test_probas[:,1])\n",
    "\n",
    "\n",
    "    grad_preds = grad_boost.predict(data.x[data.test_mask])\n",
    "\n",
    "    grad_f1 = f1_score(data.y[data.test_mask], grad_preds)\n",
    "    clf_f1 = f1_score(data.y[data.test_mask], clf_preds)\n",
    "\n",
    "\n",
    "    print(f'Decision Tree test Acc {clf_val_acc} and test AUC {clf_val_auc} and Test F1: {clf_f1}')\n",
    "    print(f'Grad Boost test Acc {grad_test_acc} and test AUC {grad_test_auc} and Test F1: {grad_f1}')\n",
    "\n",
    "\n",
    "    with open(f'gridsearch/node/{INGREDIENT_TO_CLASSIFY}_clf_grad_boost_dict_test_{LOAD_TEST_SET}', 'wb') as f:\n",
    "        pickle.dump({'clf': {'test_acc': clf_test_acc, 'test_auc':clf_test_auc, 'test_f1':clf_f1}, 'grad': {'test_acc': grad_test_acc, 'test_auc':grad_test_auc, 'test_f1':grad_f1}}, f) \n",
    "\n",
    "    if not LOAD_TEST_SET:\n",
    "        dump(clf,f'{INGREDIENT_TO_CLASSIFY}_Decision_Tree')\n",
    "        dump(grad_boost,f'{INGREDIENT_TO_CLASSIFY}_Grad_Boost')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
